{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing is a fascinating field of study that has been evolving rapidly over the past few decades.\",\n",
    "    \"Machine learning provides powerful tools for automating tasks and making predictions from data.\",\n",
    "    \"Text data is often messy and unstructured, which makes it challenging to analyze and understand without the right tools.\",\n",
    "    \"Deep learning models have shown remarkable success in understanding complex patterns in data, especially for tasks related to NLP.\",\n",
    "    \"I love building machine learning models and experimenting with different techniques to improve their performance.\",\n",
    "    \"Clean and properly preprocessed data is essential for building successful machine learning models that generalize well.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural',\n",
      "  'language',\n",
      "  'processing',\n",
      "  'fascinating',\n",
      "  'field',\n",
      "  'study',\n",
      "  'evolving',\n",
      "  'rapidly',\n",
      "  'past',\n",
      "  'decades'],\n",
      " ['machine',\n",
      "  'learning',\n",
      "  'provides',\n",
      "  'powerful',\n",
      "  'tools',\n",
      "  'automating',\n",
      "  'tasks',\n",
      "  'making',\n",
      "  'predictions',\n",
      "  'data'],\n",
      " ['text',\n",
      "  'data',\n",
      "  'messy',\n",
      "  'unstructured',\n",
      "  'makes',\n",
      "  'challenging',\n",
      "  'analyze',\n",
      "  'understand',\n",
      "  'right',\n",
      "  'tools'],\n",
      " ['deep',\n",
      "  'learning',\n",
      "  'models',\n",
      "  'shown',\n",
      "  'remarkable',\n",
      "  'success',\n",
      "  'understanding',\n",
      "  'complex',\n",
      "  'patterns',\n",
      "  'data',\n",
      "  'especially',\n",
      "  'tasks',\n",
      "  'related',\n",
      "  'nlp'],\n",
      " ['love',\n",
      "  'building',\n",
      "  'machine',\n",
      "  'learning',\n",
      "  'models',\n",
      "  'experimenting',\n",
      "  'different',\n",
      "  'techniques',\n",
      "  'improve',\n",
      "  'performance'],\n",
      " ['clean',\n",
      "  'properly',\n",
      "  'preprocessed',\n",
      "  'data',\n",
      "  'essential',\n",
      "  'building',\n",
      "  'successful',\n",
      "  'machine',\n",
      "  'learning',\n",
      "  'models',\n",
      "  'generalize']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def clean_text(documents: list[str]):\n",
    "    cleaned_docs = []\n",
    "    for doc in documents:\n",
    "        doc = nlp(re.sub(r\"[^\\w\\s]\", \"\", doc.lower()))\n",
    "        filtered_text = [token.text for token in doc if not token.is_stop]\n",
    "        cleaned_docs.append(filtered_text)\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "cleaned_corpus = clean_text(corpus)\n",
    "pprint(cleaned_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 0, 'language': 1, 'processing': 2, 'fascinating': 3, 'field': 4, 'study': 5, 'evolving': 6, 'rapidly': 7, 'past': 8, 'decades': 9, 'machine': 10, 'learning': 11, 'provides': 12, 'powerful': 13, 'tools': 14, 'automating': 15, 'tasks': 16, 'making': 17, 'predictions': 18, 'data': 19, 'text': 20, 'messy': 21, 'unstructured': 22, 'makes': 23, 'challenging': 24, 'analyze': 25, 'understand': 26, 'right': 27, 'deep': 28, 'models': 29, 'shown': 30, 'remarkable': 31, 'success': 32, 'understanding': 33, 'complex': 34, 'patterns': 35, 'especially': 36, 'related': 37, 'nlp': 38, 'love': 39, 'building': 40, 'experimenting': 41, 'different': 42, 'techniques': 43, 'improve': 44, 'performance': 45, 'clean': 46, 'properly': 47, 'preprocessed': 48, 'essential': 49, 'successful': 50, 'generalize': 51}\n",
      "{0: 'natural', 1: 'language', 2: 'processing', 3: 'fascinating', 4: 'field', 5: 'study', 6: 'evolving', 7: 'rapidly', 8: 'past', 9: 'decades', 10: 'machine', 11: 'learning', 12: 'provides', 13: 'powerful', 14: 'tools', 15: 'automating', 16: 'tasks', 17: 'making', 18: 'predictions', 19: 'data', 20: 'text', 21: 'messy', 22: 'unstructured', 23: 'makes', 24: 'challenging', 25: 'analyze', 26: 'understand', 27: 'right', 28: 'deep', 29: 'models', 30: 'shown', 31: 'remarkable', 32: 'success', 33: 'understanding', 34: 'complex', 35: 'patterns', 36: 'especially', 37: 'related', 38: 'nlp', 39: 'love', 40: 'building', 41: 'experimenting', 42: 'different', 43: 'techniques', 44: 'improve', 45: 'performance', 46: 'clean', 47: 'properly', 48: 'preprocessed', 49: 'essential', 50: 'successful', 51: 'generalize'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus: list[str]):\n",
    "    vocab = Counter(term for doc in corpus for term in doc)\n",
    "    word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "    idx_to_word = {idx: word for idx, (word, _) in enumerate(vocab.items())}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "word_to_idx, idx_to_word = build_vocab(cleaned_corpus)\n",
    "print(word_to_idx)\n",
    "print(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['natural', 'language', 'processing'], 'natural'),\n",
       " (['natural', 'language', 'processing', 'fascinating'], 'language'),\n",
       " (['natural', 'language', 'processing', 'fascinating', 'field'], 'processing'),\n",
       " (['language', 'processing', 'fascinating', 'field', 'study'], 'fascinating'),\n",
       " (['processing', 'fascinating', 'field', 'study', 'evolving'], 'field'),\n",
       " (['fascinating', 'field', 'study', 'evolving', 'rapidly'], 'study'),\n",
       " (['field', 'study', 'evolving', 'rapidly', 'past'], 'evolving'),\n",
       " (['study', 'evolving', 'rapidly', 'past', 'decades'], 'rapidly'),\n",
       " (['evolving', 'rapidly', 'past', 'decades'], 'past'),\n",
       " (['rapidly', 'past', 'decades'], 'decades'),\n",
       " (['machine', 'learning', 'provides'], 'machine'),\n",
       " (['machine', 'learning', 'provides', 'powerful'], 'learning'),\n",
       " (['machine', 'learning', 'provides', 'powerful', 'tools'], 'provides'),\n",
       " (['learning', 'provides', 'powerful', 'tools', 'automating'], 'powerful'),\n",
       " (['provides', 'powerful', 'tools', 'automating', 'tasks'], 'tools'),\n",
       " (['powerful', 'tools', 'automating', 'tasks', 'making'], 'automating'),\n",
       " (['tools', 'automating', 'tasks', 'making', 'predictions'], 'tasks'),\n",
       " (['automating', 'tasks', 'making', 'predictions', 'data'], 'making'),\n",
       " (['tasks', 'making', 'predictions', 'data'], 'predictions'),\n",
       " (['making', 'predictions', 'data'], 'data'),\n",
       " (['text', 'data', 'messy'], 'text'),\n",
       " (['text', 'data', 'messy', 'unstructured'], 'data'),\n",
       " (['text', 'data', 'messy', 'unstructured', 'makes'], 'messy'),\n",
       " (['data', 'messy', 'unstructured', 'makes', 'challenging'], 'unstructured'),\n",
       " (['messy', 'unstructured', 'makes', 'challenging', 'analyze'], 'makes'),\n",
       " (['unstructured', 'makes', 'challenging', 'analyze', 'understand'],\n",
       "  'challenging'),\n",
       " (['makes', 'challenging', 'analyze', 'understand', 'right'], 'analyze'),\n",
       " (['challenging', 'analyze', 'understand', 'right', 'tools'], 'understand'),\n",
       " (['analyze', 'understand', 'right', 'tools'], 'right'),\n",
       " (['understand', 'right', 'tools'], 'tools'),\n",
       " (['deep', 'learning', 'models'], 'deep'),\n",
       " (['deep', 'learning', 'models', 'shown'], 'learning'),\n",
       " (['deep', 'learning', 'models', 'shown', 'remarkable'], 'models'),\n",
       " (['learning', 'models', 'shown', 'remarkable', 'success'], 'shown'),\n",
       " (['models', 'shown', 'remarkable', 'success', 'understanding'], 'remarkable'),\n",
       " (['shown', 'remarkable', 'success', 'understanding', 'complex'], 'success'),\n",
       " (['remarkable', 'success', 'understanding', 'complex', 'patterns'],\n",
       "  'understanding'),\n",
       " (['success', 'understanding', 'complex', 'patterns', 'data'], 'complex'),\n",
       " (['understanding', 'complex', 'patterns', 'data', 'especially'], 'patterns'),\n",
       " (['complex', 'patterns', 'data', 'especially', 'tasks'], 'data'),\n",
       " (['patterns', 'data', 'especially', 'tasks', 'related'], 'especially'),\n",
       " (['data', 'especially', 'tasks', 'related', 'nlp'], 'tasks'),\n",
       " (['especially', 'tasks', 'related', 'nlp'], 'related'),\n",
       " (['tasks', 'related', 'nlp'], 'nlp'),\n",
       " (['love', 'building', 'machine'], 'love'),\n",
       " (['love', 'building', 'machine', 'learning'], 'building'),\n",
       " (['love', 'building', 'machine', 'learning', 'models'], 'machine'),\n",
       " (['building', 'machine', 'learning', 'models', 'experimenting'], 'learning'),\n",
       " (['machine', 'learning', 'models', 'experimenting', 'different'], 'models'),\n",
       " (['learning', 'models', 'experimenting', 'different', 'techniques'],\n",
       "  'experimenting'),\n",
       " (['models', 'experimenting', 'different', 'techniques', 'improve'],\n",
       "  'different'),\n",
       " (['experimenting', 'different', 'techniques', 'improve', 'performance'],\n",
       "  'techniques'),\n",
       " (['different', 'techniques', 'improve', 'performance'], 'improve'),\n",
       " (['techniques', 'improve', 'performance'], 'performance'),\n",
       " (['clean', 'properly', 'preprocessed'], 'clean'),\n",
       " (['clean', 'properly', 'preprocessed', 'data'], 'properly'),\n",
       " (['clean', 'properly', 'preprocessed', 'data', 'essential'], 'preprocessed'),\n",
       " (['properly', 'preprocessed', 'data', 'essential', 'building'], 'data'),\n",
       " (['preprocessed', 'data', 'essential', 'building', 'successful'],\n",
       "  'essential'),\n",
       " (['data', 'essential', 'building', 'successful', 'machine'], 'building'),\n",
       " (['essential', 'building', 'successful', 'machine', 'learning'],\n",
       "  'successful'),\n",
       " (['building', 'successful', 'machine', 'learning', 'models'], 'machine'),\n",
       " (['successful', 'machine', 'learning', 'models', 'generalize'], 'learning'),\n",
       " (['machine', 'learning', 'models', 'generalize'], 'models'),\n",
       " (['learning', 'models', 'generalize'], 'generalize')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_context_target_pairs(corpus: list[str], window_size: int = 2):\n",
    "    pairs = []\n",
    "    for document in corpus:\n",
    "        for idx, term in enumerate(document):\n",
    "            start_idx = max(idx - window_size, 0)\n",
    "            end_idx = min(idx + window_size + 1, len(document))\n",
    "            pairs.append(([document[i] for i in range(start_idx, end_idx)], term))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = create_context_target_pairs(cleaned_corpus)\n",
    "pairs           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1, 2], 0),\n",
       " ([0, 1, 2, 3], 1),\n",
       " ([0, 1, 2, 3, 4], 2),\n",
       " ([1, 2, 3, 4, 5], 3),\n",
       " ([2, 3, 4, 5, 6], 4),\n",
       " ([3, 4, 5, 6, 7], 5),\n",
       " ([4, 5, 6, 7, 8], 6),\n",
       " ([5, 6, 7, 8, 9], 7),\n",
       " ([6, 7, 8, 9], 8),\n",
       " ([7, 8, 9], 9),\n",
       " ([10, 11, 12], 10),\n",
       " ([10, 11, 12, 13], 11),\n",
       " ([10, 11, 12, 13, 14], 12),\n",
       " ([11, 12, 13, 14, 15], 13),\n",
       " ([12, 13, 14, 15, 16], 14),\n",
       " ([13, 14, 15, 16, 17], 15),\n",
       " ([14, 15, 16, 17, 18], 16),\n",
       " ([15, 16, 17, 18, 19], 17),\n",
       " ([16, 17, 18, 19], 18),\n",
       " ([17, 18, 19], 19),\n",
       " ([20, 19, 21], 20),\n",
       " ([20, 19, 21, 22], 19),\n",
       " ([20, 19, 21, 22, 23], 21),\n",
       " ([19, 21, 22, 23, 24], 22),\n",
       " ([21, 22, 23, 24, 25], 23),\n",
       " ([22, 23, 24, 25, 26], 24),\n",
       " ([23, 24, 25, 26, 27], 25),\n",
       " ([24, 25, 26, 27, 14], 26),\n",
       " ([25, 26, 27, 14], 27),\n",
       " ([26, 27, 14], 14),\n",
       " ([28, 11, 29], 28),\n",
       " ([28, 11, 29, 30], 11),\n",
       " ([28, 11, 29, 30, 31], 29),\n",
       " ([11, 29, 30, 31, 32], 30),\n",
       " ([29, 30, 31, 32, 33], 31),\n",
       " ([30, 31, 32, 33, 34], 32),\n",
       " ([31, 32, 33, 34, 35], 33),\n",
       " ([32, 33, 34, 35, 19], 34),\n",
       " ([33, 34, 35, 19, 36], 35),\n",
       " ([34, 35, 19, 36, 16], 19),\n",
       " ([35, 19, 36, 16, 37], 36),\n",
       " ([19, 36, 16, 37, 38], 16),\n",
       " ([36, 16, 37, 38], 37),\n",
       " ([16, 37, 38], 38),\n",
       " ([39, 40, 10], 39),\n",
       " ([39, 40, 10, 11], 40),\n",
       " ([39, 40, 10, 11, 29], 10),\n",
       " ([40, 10, 11, 29, 41], 11),\n",
       " ([10, 11, 29, 41, 42], 29),\n",
       " ([11, 29, 41, 42, 43], 41),\n",
       " ([29, 41, 42, 43, 44], 42),\n",
       " ([41, 42, 43, 44, 45], 43),\n",
       " ([42, 43, 44, 45], 44),\n",
       " ([43, 44, 45], 45),\n",
       " ([46, 47, 48], 46),\n",
       " ([46, 47, 48, 19], 47),\n",
       " ([46, 47, 48, 19, 49], 48),\n",
       " ([47, 48, 19, 49, 40], 19),\n",
       " ([48, 19, 49, 40, 50], 49),\n",
       " ([19, 49, 40, 50, 10], 40),\n",
       " ([49, 40, 50, 10, 11], 50),\n",
       " ([40, 50, 10, 11, 29], 10),\n",
       " ([50, 10, 11, 29, 51], 11),\n",
       " ([10, 11, 29, 51], 29),\n",
       " ([11, 29, 51], 51)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_pairs(pairs: list[str], word_to_idx: dict):\n",
    "    encoded_pairs = []\n",
    "    for context, target in pairs:\n",
    "        context_idx = [word_to_idx[term] for term in context]\n",
    "        target_idx = word_to_idx[target]\n",
    "        encoded_pairs.append((context_idx, target_idx))\n",
    "\n",
    "    return encoded_pairs\n",
    "\n",
    "encoded_pairs = encode_pairs(pairs, word_to_idx)\n",
    "encoded_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[260.90443658828735,\n",
       " 258.69257378578186,\n",
       " 256.5143029689789,\n",
       " 254.36835098266602,\n",
       " 252.25356817245483,\n",
       " 250.16892910003662,\n",
       " 248.11352348327637,\n",
       " 246.0865547657013,\n",
       " 244.08734774589539,\n",
       " 242.11532282829285,\n",
       " 240.17001032829285,\n",
       " 238.25102734565735,\n",
       " 236.3580801486969,\n",
       " 234.49096059799194,\n",
       " 232.64952182769775,\n",
       " 230.83367776870728,\n",
       " 229.0433909893036,\n",
       " 227.27866196632385,\n",
       " 225.53951120376587,\n",
       " 223.82597541809082,\n",
       " 222.1380751132965,\n",
       " 220.47582972049713,\n",
       " 218.83922016620636,\n",
       " 217.2281894683838,\n",
       " 215.642636179924,\n",
       " 214.08240628242493,\n",
       " 212.5472673177719,\n",
       " 211.03693866729736,\n",
       " 209.5510617494583,\n",
       " 208.089213848114,\n",
       " 206.65091395378113,\n",
       " 205.2356197834015,\n",
       " 203.8427346944809,\n",
       " 202.4716159105301,\n",
       " 201.12159132957458,\n",
       " 199.79195046424866,\n",
       " 198.48197078704834,\n",
       " 197.19091093540192,\n",
       " 195.91802787780762,\n",
       " 194.66257762908936,\n",
       " 193.42382884025574,\n",
       " 192.2010635137558,\n",
       " 190.99357795715332,\n",
       " 189.80069839954376,\n",
       " 188.6217725276947,\n",
       " 187.45617401599884,\n",
       " 186.30331897735596,\n",
       " 185.1626352071762,\n",
       " 184.03359591960907,\n",
       " 182.9156974554062,\n",
       " 181.80847787857056,\n",
       " 180.71149277687073,\n",
       " 179.6243405342102,\n",
       " 178.54663932323456,\n",
       " 177.47803580760956,\n",
       " 176.41821265220642,\n",
       " 175.36686372756958,\n",
       " 174.32371830940247,\n",
       " 173.28851771354675,\n",
       " 172.26103395223618,\n",
       " 171.24104684591293,\n",
       " 170.22836178541183,\n",
       " 169.22279918193817,\n",
       " 168.224196434021,\n",
       " 167.23239696025848,\n",
       " 166.2472671866417,\n",
       " 165.26867759227753,\n",
       " 164.29651176929474,\n",
       " 163.33065980672836,\n",
       " 162.37103247642517,\n",
       " 161.41753393411636,\n",
       " 160.47007942199707,\n",
       " 159.52859777212143,\n",
       " 158.5930169224739,\n",
       " 157.66326761245728,\n",
       " 156.73929953575134,\n",
       " 155.82105267047882,\n",
       " 154.90847277641296,\n",
       " 154.001522898674,\n",
       " 153.10014832019806,\n",
       " 152.20431125164032,\n",
       " 151.31398463249207,\n",
       " 150.4291200041771,\n",
       " 149.54969042539597,\n",
       " 148.67567044496536,\n",
       " 147.80702924728394,\n",
       " 146.94374126195908,\n",
       " 146.08578157424927,\n",
       " 145.23313331604004,\n",
       " 144.38577157258987,\n",
       " 143.54367965459824,\n",
       " 142.70684212446213,\n",
       " 141.87524509429932,\n",
       " 141.04887157678604,\n",
       " 140.22771161794662,\n",
       " 139.41174787282944,\n",
       " 138.60097539424896,\n",
       " 137.79538440704346,\n",
       " 136.99496245384216,\n",
       " 136.1997076869011]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embedded = self.embeddings(context).mean(dim=1)\n",
    "        out = self.linear(embedded)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, encoded_pairs, word_to_idx, epochs, learning_rate):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in encoded_pairs:\n",
    "            context_tensor = torch.tensor([context], dtype=torch.long)\n",
    "            target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(context_tensor)\n",
    "            loss = loss_function(output, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        losses.append(total_loss)\n",
    "\n",
    "    return losses\n",
    "\n",
    "embedding_dim = 10\n",
    "model = CBOWModel(vocab_size=len(word_to_idx), embedding_dim=embedding_dim)\n",
    "losses = train_model(model, encoded_pairs, word_to_idx, epochs=100, learning_rate=0.01)\n",
    "losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
