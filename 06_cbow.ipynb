{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Natural Language Processing is a fascinating field of study that has been evolving rapidly over the past few decades.\",\n",
    "    \"Machine learning provides powerful tools for automating tasks and making predictions from data.\",\n",
    "    \"Text data is often messy and unstructured, which makes it challenging to analyze and understand without the right tools.\",\n",
    "    \"Deep learning models have shown remarkable success in understanding complex patterns in data, especially for tasks related to NLP.\",\n",
    "    \"I love building machine learning models and experimenting with different techniques to improve their performance.\",\n",
    "    \"Clean and properly preprocessed data is essential for building successful machine learning models that generalize well.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\",\n",
       " 'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.',\n",
       " 'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.',\n",
       " \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\",\n",
       " 'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"imdb_reviews.csv\")\n",
    "corpus = df['review'].tolist()\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['reviewers',\n",
       "  'mentioned',\n",
       "  'watching',\n",
       "  '1',\n",
       "  'oz',\n",
       "  'episode',\n",
       "  'll',\n",
       "  'hooked',\n",
       "  'right',\n",
       "  'exactly',\n",
       "  'happened',\n",
       "  'mebr',\n",
       "  'br',\n",
       "  'thing',\n",
       "  'struck',\n",
       "  'oz',\n",
       "  'brutality',\n",
       "  'unflinching',\n",
       "  'scenes',\n",
       "  'violence',\n",
       "  'set',\n",
       "  'right',\n",
       "  'word',\n",
       "  'trust',\n",
       "  'faint',\n",
       "  'hearted',\n",
       "  'timid',\n",
       "  'pulls',\n",
       "  'punches',\n",
       "  'regards',\n",
       "  'drugs',\n",
       "  'sex',\n",
       "  'violence',\n",
       "  'hardcore',\n",
       "  'classic',\n",
       "  'use',\n",
       "  'wordbr',\n",
       "  'br',\n",
       "  'called',\n",
       "  'oz',\n",
       "  'nickname',\n",
       "  'given',\n",
       "  'oswald',\n",
       "  'maximum',\n",
       "  'security',\n",
       "  'state',\n",
       "  'penitentary',\n",
       "  'focuses',\n",
       "  'mainly',\n",
       "  'emerald',\n",
       "  'city',\n",
       "  'experimental',\n",
       "  'section',\n",
       "  'prison',\n",
       "  'cells',\n",
       "  'glass',\n",
       "  'fronts',\n",
       "  'face',\n",
       "  'inwards',\n",
       "  'privacy',\n",
       "  'high',\n",
       "  'agenda',\n",
       "  'em',\n",
       "  'city',\n",
       "  'home',\n",
       "  'manyaryans',\n",
       "  'muslims',\n",
       "  'gangstas',\n",
       "  'latinos',\n",
       "  'christians',\n",
       "  'italians',\n",
       "  'irish',\n",
       "  'moreso',\n",
       "  'scuffles',\n",
       "  'death',\n",
       "  'stares',\n",
       "  'dodgy',\n",
       "  'dealings',\n",
       "  'shady',\n",
       "  'agreements',\n",
       "  'far',\n",
       "  'awaybr',\n",
       "  'br',\n",
       "  'main',\n",
       "  'appeal',\n",
       "  'fact',\n",
       "  'goes',\n",
       "  'shows',\n",
       "  'nt',\n",
       "  'dare',\n",
       "  'forget',\n",
       "  'pretty',\n",
       "  'pictures',\n",
       "  'painted',\n",
       "  'mainstream',\n",
       "  'audiences',\n",
       "  'forget',\n",
       "  'charm',\n",
       "  'forget',\n",
       "  'romanceoz',\n",
       "  'nt',\n",
       "  'mess',\n",
       "  'episode',\n",
       "  'saw',\n",
       "  'struck',\n",
       "  'nasty',\n",
       "  'surreal',\n",
       "  'nt',\n",
       "  'ready',\n",
       "  'watched',\n",
       "  'developed',\n",
       "  'taste',\n",
       "  'oz',\n",
       "  'got',\n",
       "  'accustomed',\n",
       "  'high',\n",
       "  'levels',\n",
       "  'graphic',\n",
       "  'violence',\n",
       "  'violence',\n",
       "  'injustice',\n",
       "  'crooked',\n",
       "  'guards',\n",
       "  'll',\n",
       "  'sold',\n",
       "  'nickel',\n",
       "  'inmates',\n",
       "  'll',\n",
       "  'kill',\n",
       "  'order',\n",
       "  'away',\n",
       "  'mannered',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'inmates',\n",
       "  'turned',\n",
       "  'prison',\n",
       "  'bitches',\n",
       "  'lack',\n",
       "  'street',\n",
       "  'skills',\n",
       "  'prison',\n",
       "  'experience',\n",
       "  'watching',\n",
       "  'oz',\n",
       "  'comfortable',\n",
       "  'uncomfortable',\n",
       "  'viewingthats',\n",
       "  'touch',\n",
       "  'darker'],\n",
       " ['wonderful',\n",
       "  'little',\n",
       "  'production',\n",
       "  'br',\n",
       "  'br',\n",
       "  'filming',\n",
       "  'technique',\n",
       "  'unassuming',\n",
       "  'oldtimebbc',\n",
       "  'fashion',\n",
       "  'gives',\n",
       "  'comforting',\n",
       "  'discomforting',\n",
       "  'sense',\n",
       "  'realism',\n",
       "  'entire',\n",
       "  'piece',\n",
       "  'br',\n",
       "  'br',\n",
       "  'actors',\n",
       "  'extremely',\n",
       "  'chosen',\n",
       "  'michael',\n",
       "  'sheen',\n",
       "  'got',\n",
       "  'polari',\n",
       "  'voices',\n",
       "  'pat',\n",
       "  'truly',\n",
       "  'seamless',\n",
       "  'editing',\n",
       "  'guided',\n",
       "  'references',\n",
       "  'williams',\n",
       "  'diary',\n",
       "  'entries',\n",
       "  'worth',\n",
       "  'watching',\n",
       "  'terrificly',\n",
       "  'written',\n",
       "  'performed',\n",
       "  'piece',\n",
       "  'masterful',\n",
       "  'production',\n",
       "  'great',\n",
       "  'masters',\n",
       "  'comedy',\n",
       "  'life',\n",
       "  'br',\n",
       "  'br',\n",
       "  'realism',\n",
       "  'comes',\n",
       "  'home',\n",
       "  'little',\n",
       "  'things',\n",
       "  'fantasy',\n",
       "  'guard',\n",
       "  'use',\n",
       "  'traditional',\n",
       "  'dream',\n",
       "  'techniques',\n",
       "  'remains',\n",
       "  'solid',\n",
       "  'disappears',\n",
       "  'plays',\n",
       "  'knowledge',\n",
       "  'senses',\n",
       "  'particularly',\n",
       "  'scenes',\n",
       "  'concerning',\n",
       "  'orton',\n",
       "  'halliwell',\n",
       "  'sets',\n",
       "  'particularly',\n",
       "  'flat',\n",
       "  'halliwells',\n",
       "  'murals',\n",
       "  'decorating',\n",
       "  'surface',\n",
       "  'terribly'],\n",
       " ['thought',\n",
       "  'wonderful',\n",
       "  'way',\n",
       "  'spend',\n",
       "  'time',\n",
       "  'hot',\n",
       "  'summer',\n",
       "  'weekend',\n",
       "  'sitting',\n",
       "  'air',\n",
       "  'conditioned',\n",
       "  'theater',\n",
       "  'watching',\n",
       "  'lighthearted',\n",
       "  'comedy',\n",
       "  'plot',\n",
       "  'simplistic',\n",
       "  'dialogue',\n",
       "  'witty',\n",
       "  'characters',\n",
       "  'likable',\n",
       "  'bread',\n",
       "  'suspected',\n",
       "  'serial',\n",
       "  'killer',\n",
       "  'disappointed',\n",
       "  'realize',\n",
       "  'match',\n",
       "  'point',\n",
       "  '2',\n",
       "  'risk',\n",
       "  'addiction',\n",
       "  'thought',\n",
       "  'proof',\n",
       "  'woody',\n",
       "  'allen',\n",
       "  'fully',\n",
       "  'control',\n",
       "  'style',\n",
       "  'grown',\n",
       "  'lovebr',\n",
       "  'br',\n",
       "  'd',\n",
       "  'laughed',\n",
       "  'woodys',\n",
       "  'comedies',\n",
       "  'years',\n",
       "  'dare',\n",
       "  'decade',\n",
       "  've',\n",
       "  'impressed',\n",
       "  'scarlet',\n",
       "  'johanson',\n",
       "  'managed',\n",
       "  'tone',\n",
       "  'sexy',\n",
       "  'image',\n",
       "  'jumped',\n",
       "  'right',\n",
       "  'average',\n",
       "  'spirited',\n",
       "  'young',\n",
       "  'womanbr',\n",
       "  'br',\n",
       "  'crown',\n",
       "  'jewel',\n",
       "  'career',\n",
       "  'wittier',\n",
       "  'devil',\n",
       "  'wears',\n",
       "  'prada',\n",
       "  'interesting',\n",
       "  'superman',\n",
       "  'great',\n",
       "  'comedy',\n",
       "  'friends'],\n",
       " ['basically',\n",
       "  's',\n",
       "  'family',\n",
       "  'little',\n",
       "  'boy',\n",
       "  'jake',\n",
       "  'thinks',\n",
       "  's',\n",
       "  'zombie',\n",
       "  'closet',\n",
       "  'parents',\n",
       "  'fighting',\n",
       "  'timebr',\n",
       "  'br',\n",
       "  'movie',\n",
       "  'slower',\n",
       "  'soap',\n",
       "  'opera',\n",
       "  'suddenly',\n",
       "  'jake',\n",
       "  'decides',\n",
       "  'rambo',\n",
       "  'kill',\n",
       "  'zombiebr',\n",
       "  'br',\n",
       "  'ok',\n",
       "  'going',\n",
       "  'film',\n",
       "  'decide',\n",
       "  'thriller',\n",
       "  'drama',\n",
       "  'drama',\n",
       "  'movie',\n",
       "  'watchable',\n",
       "  'parents',\n",
       "  'divorcing',\n",
       "  'arguing',\n",
       "  'like',\n",
       "  'real',\n",
       "  'life',\n",
       "  'jake',\n",
       "  'closet',\n",
       "  'totally',\n",
       "  'ruins',\n",
       "  'film',\n",
       "  'expected',\n",
       "  'boogeyman',\n",
       "  'similar',\n",
       "  'movie',\n",
       "  'instead',\n",
       "  'watched',\n",
       "  'drama',\n",
       "  'meaningless',\n",
       "  'thriller',\n",
       "  'spotsbr',\n",
       "  'br',\n",
       "  '3',\n",
       "  '10',\n",
       "  'playing',\n",
       "  'parents',\n",
       "  'descent',\n",
       "  'dialogs',\n",
       "  'shots',\n",
       "  'jake',\n",
       "  'ignore'],\n",
       " ['petter',\n",
       "  'matteis',\n",
       "  'love',\n",
       "  'time',\n",
       "  'money',\n",
       "  'visually',\n",
       "  'stunning',\n",
       "  'film',\n",
       "  'watch',\n",
       "  'mr',\n",
       "  'mattei',\n",
       "  'offers',\n",
       "  'vivid',\n",
       "  'portrait',\n",
       "  'human',\n",
       "  'relations',\n",
       "  'movie',\n",
       "  'telling',\n",
       "  'money',\n",
       "  'power',\n",
       "  'success',\n",
       "  'people',\n",
       "  'different',\n",
       "  'situations',\n",
       "  'encounter',\n",
       "  'br',\n",
       "  'br',\n",
       "  'variation',\n",
       "  'arthur',\n",
       "  'schnitzlers',\n",
       "  'play',\n",
       "  'theme',\n",
       "  'director',\n",
       "  'transfers',\n",
       "  'action',\n",
       "  'present',\n",
       "  'time',\n",
       "  'new',\n",
       "  'york',\n",
       "  'different',\n",
       "  'characters',\n",
       "  'meet',\n",
       "  'connect',\n",
       "  'connected',\n",
       "  'way',\n",
       "  'person',\n",
       "  'know',\n",
       "  'previous',\n",
       "  'point',\n",
       "  'contact',\n",
       "  'stylishly',\n",
       "  'film',\n",
       "  'sophisticated',\n",
       "  'luxurious',\n",
       "  'look',\n",
       "  'taken',\n",
       "  'people',\n",
       "  'live',\n",
       "  'world',\n",
       "  'live',\n",
       "  'habitatbr',\n",
       "  'br',\n",
       "  'thing',\n",
       "  'gets',\n",
       "  'souls',\n",
       "  'picture',\n",
       "  'different',\n",
       "  'stages',\n",
       "  'loneliness',\n",
       "  'inhabits',\n",
       "  'big',\n",
       "  'city',\n",
       "  'exactly',\n",
       "  'best',\n",
       "  'place',\n",
       "  'human',\n",
       "  'relations',\n",
       "  'find',\n",
       "  'sincere',\n",
       "  'fulfillment',\n",
       "  'discerns',\n",
       "  'case',\n",
       "  'people',\n",
       "  'encounterbr',\n",
       "  'br',\n",
       "  'acting',\n",
       "  'good',\n",
       "  'mr',\n",
       "  'matteis',\n",
       "  'direction',\n",
       "  'steve',\n",
       "  'buscemi',\n",
       "  'rosario',\n",
       "  'dawson',\n",
       "  'carol',\n",
       "  'kane',\n",
       "  'michael',\n",
       "  'imperioli',\n",
       "  'adrian',\n",
       "  'grenier',\n",
       "  'rest',\n",
       "  'talented',\n",
       "  'cast',\n",
       "  'characters',\n",
       "  'come',\n",
       "  'alivebr',\n",
       "  'br',\n",
       "  'wish',\n",
       "  'mr',\n",
       "  'mattei',\n",
       "  'good',\n",
       "  'luck',\n",
       "  'await',\n",
       "  'anxiously',\n",
       "  'work']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(documents: list[str], batch_size: int = 100):\n",
    "\n",
    "    pattern = re.compile(r\"[^\\w\\s]\")\n",
    "    batch_size = batch_size\n",
    "\n",
    "    cleaned_docs = []\n",
    "    for i in range (0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        docs_text = [pattern.sub(\"\", doc.lower()) for doc in batch_docs]\n",
    "        for doc_nlp in nlp.pipe(docs_text):\n",
    "            filtered_text = [token.text for token in doc_nlp if not token.is_stop and token.text.strip()]\n",
    "            cleaned_docs.append(filtered_text)\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "cleaned_corpus = clean_text(corpus[:5])\n",
    "cleaned_corpus[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewers': 0, 'mentioned': 1, 'watching': 2, '1': 3, 'oz': 4, 'episode': 5, 'll': 6, 'hooked': 7, 'right': 8, 'exactly': 9, 'happened': 10, 'mebr': 11, 'br': 12, 'thing': 13, 'struck': 14, 'brutality': 15, 'unflinching': 16, 'scenes': 17, 'violence': 18, 'set': 19, 'word': 20, 'trust': 21, 'faint': 22, 'hearted': 23, 'timid': 24, 'pulls': 25, 'punches': 26, 'regards': 27, 'drugs': 28, 'sex': 29, 'hardcore': 30, 'classic': 31, 'use': 32, 'wordbr': 33, 'called': 34, 'nickname': 35, 'given': 36, 'oswald': 37, 'maximum': 38, 'security': 39, 'state': 40, 'penitentary': 41, 'focuses': 42, 'mainly': 43, 'emerald': 44, 'city': 45, 'experimental': 46, 'section': 47, 'prison': 48, 'cells': 49, 'glass': 50, 'fronts': 51, 'face': 52, 'inwards': 53, 'privacy': 54, 'high': 55, 'agenda': 56, 'em': 57, 'home': 58, 'manyaryans': 59, 'muslims': 60, 'gangstas': 61, 'latinos': 62, 'christians': 63, 'italians': 64, 'irish': 65, 'moreso': 66, 'scuffles': 67, 'death': 68, 'stares': 69, 'dodgy': 70, 'dealings': 71, 'shady': 72, 'agreements': 73, 'far': 74, 'awaybr': 75, 'main': 76, 'appeal': 77, 'fact': 78, 'goes': 79, 'shows': 80, 'nt': 81, 'dare': 82, 'forget': 83, 'pretty': 84, 'pictures': 85, 'painted': 86, 'mainstream': 87, 'audiences': 88, 'charm': 89, 'romanceoz': 90, 'mess': 91, 'saw': 92, 'nasty': 93, 'surreal': 94, 'ready': 95, 'watched': 96, 'developed': 97, 'taste': 98, 'got': 99, 'accustomed': 100, 'levels': 101, 'graphic': 102, 'injustice': 103, 'crooked': 104, 'guards': 105, 'sold': 106, 'nickel': 107, 'inmates': 108, 'kill': 109, 'order': 110, 'away': 111, 'mannered': 112, 'middle': 113, 'class': 114, 'turned': 115, 'bitches': 116, 'lack': 117, 'street': 118, 'skills': 119, 'experience': 120, 'comfortable': 121, 'uncomfortable': 122, 'viewingthats': 123, 'touch': 124, 'darker': 125, 'wonderful': 126, 'little': 127, 'production': 128, 'filming': 129, 'technique': 130, 'unassuming': 131, 'oldtimebbc': 132, 'fashion': 133, 'gives': 134, 'comforting': 135, 'discomforting': 136, 'sense': 137, 'realism': 138, 'entire': 139, 'piece': 140, 'actors': 141, 'extremely': 142, 'chosen': 143, 'michael': 144, 'sheen': 145, 'polari': 146, 'voices': 147, 'pat': 148, 'truly': 149, 'seamless': 150, 'editing': 151, 'guided': 152, 'references': 153, 'williams': 154, 'diary': 155, 'entries': 156, 'worth': 157, 'terrificly': 158, 'written': 159, 'performed': 160, 'masterful': 161, 'great': 162, 'masters': 163, 'comedy': 164, 'life': 165, 'comes': 166, 'things': 167, 'fantasy': 168, 'guard': 169, 'traditional': 170, 'dream': 171, 'techniques': 172, 'remains': 173, 'solid': 174, 'disappears': 175, 'plays': 176, 'knowledge': 177, 'senses': 178, 'particularly': 179, 'concerning': 180, 'orton': 181, 'halliwell': 182, 'sets': 183, 'flat': 184, 'halliwells': 185, 'murals': 186, 'decorating': 187, 'surface': 188, 'terribly': 189, 'thought': 190, 'way': 191, 'spend': 192, 'time': 193, 'hot': 194, 'summer': 195, 'weekend': 196, 'sitting': 197, 'air': 198, 'conditioned': 199, 'theater': 200, 'lighthearted': 201, 'plot': 202, 'simplistic': 203, 'dialogue': 204, 'witty': 205, 'characters': 206, 'likable': 207, 'bread': 208, 'suspected': 209, 'serial': 210, 'killer': 211, 'disappointed': 212, 'realize': 213, 'match': 214, 'point': 215, '2': 216, 'risk': 217, 'addiction': 218, 'proof': 219, 'woody': 220, 'allen': 221, 'fully': 222, 'control': 223, 'style': 224, 'grown': 225, 'lovebr': 226, 'd': 227, 'laughed': 228, 'woodys': 229, 'comedies': 230, 'years': 231, 'decade': 232, 've': 233, 'impressed': 234, 'scarlet': 235, 'johanson': 236, 'managed': 237, 'tone': 238, 'sexy': 239, 'image': 240, 'jumped': 241, 'average': 242, 'spirited': 243, 'young': 244, 'womanbr': 245, 'crown': 246, 'jewel': 247, 'career': 248, 'wittier': 249, 'devil': 250, 'wears': 251, 'prada': 252, 'interesting': 253, 'superman': 254, 'friends': 255, 'basically': 256, 's': 257, 'family': 258, 'boy': 259, 'jake': 260, 'thinks': 261, 'zombie': 262, 'closet': 263, 'parents': 264, 'fighting': 265, 'timebr': 266, 'movie': 267, 'slower': 268, 'soap': 269, 'opera': 270, 'suddenly': 271, 'decides': 272, 'rambo': 273, 'zombiebr': 274, 'ok': 275, 'going': 276, 'film': 277, 'decide': 278, 'thriller': 279, 'drama': 280, 'watchable': 281, 'divorcing': 282, 'arguing': 283, 'like': 284, 'real': 285, 'totally': 286, 'ruins': 287, 'expected': 288, 'boogeyman': 289, 'similar': 290, 'instead': 291, 'meaningless': 292, 'spotsbr': 293, '3': 294, '10': 295, 'playing': 296, 'descent': 297, 'dialogs': 298, 'shots': 299, 'ignore': 300, 'petter': 301, 'matteis': 302, 'love': 303, 'money': 304, 'visually': 305, 'stunning': 306, 'watch': 307, 'mr': 308, 'mattei': 309, 'offers': 310, 'vivid': 311, 'portrait': 312, 'human': 313, 'relations': 314, 'telling': 315, 'power': 316, 'success': 317, 'people': 318, 'different': 319, 'situations': 320, 'encounter': 321, 'variation': 322, 'arthur': 323, 'schnitzlers': 324, 'play': 325, 'theme': 326, 'director': 327, 'transfers': 328, 'action': 329, 'present': 330, 'new': 331, 'york': 332, 'meet': 333, 'connect': 334, 'connected': 335, 'person': 336, 'know': 337, 'previous': 338, 'contact': 339, 'stylishly': 340, 'sophisticated': 341, 'luxurious': 342, 'look': 343, 'taken': 344, 'live': 345, 'world': 346, 'habitatbr': 347, 'gets': 348, 'souls': 349, 'picture': 350, 'stages': 351, 'loneliness': 352, 'inhabits': 353, 'big': 354, 'best': 355, 'place': 356, 'find': 357, 'sincere': 358, 'fulfillment': 359, 'discerns': 360, 'case': 361, 'encounterbr': 362, 'acting': 363, 'good': 364, 'direction': 365, 'steve': 366, 'buscemi': 367, 'rosario': 368, 'dawson': 369, 'carol': 370, 'kane': 371, 'imperioli': 372, 'adrian': 373, 'grenier': 374, 'rest': 375, 'talented': 376, 'cast': 377, 'come': 378, 'alivebr': 379, 'wish': 380, 'luck': 381, 'await': 382, 'anxiously': 383, 'work': 384}\n",
      "{0: 'reviewers', 1: 'mentioned', 2: 'watching', 3: '1', 4: 'oz', 5: 'episode', 6: 'll', 7: 'hooked', 8: 'right', 9: 'exactly', 10: 'happened', 11: 'mebr', 12: 'br', 13: 'thing', 14: 'struck', 15: 'brutality', 16: 'unflinching', 17: 'scenes', 18: 'violence', 19: 'set', 20: 'word', 21: 'trust', 22: 'faint', 23: 'hearted', 24: 'timid', 25: 'pulls', 26: 'punches', 27: 'regards', 28: 'drugs', 29: 'sex', 30: 'hardcore', 31: 'classic', 32: 'use', 33: 'wordbr', 34: 'called', 35: 'nickname', 36: 'given', 37: 'oswald', 38: 'maximum', 39: 'security', 40: 'state', 41: 'penitentary', 42: 'focuses', 43: 'mainly', 44: 'emerald', 45: 'city', 46: 'experimental', 47: 'section', 48: 'prison', 49: 'cells', 50: 'glass', 51: 'fronts', 52: 'face', 53: 'inwards', 54: 'privacy', 55: 'high', 56: 'agenda', 57: 'em', 58: 'home', 59: 'manyaryans', 60: 'muslims', 61: 'gangstas', 62: 'latinos', 63: 'christians', 64: 'italians', 65: 'irish', 66: 'moreso', 67: 'scuffles', 68: 'death', 69: 'stares', 70: 'dodgy', 71: 'dealings', 72: 'shady', 73: 'agreements', 74: 'far', 75: 'awaybr', 76: 'main', 77: 'appeal', 78: 'fact', 79: 'goes', 80: 'shows', 81: 'nt', 82: 'dare', 83: 'forget', 84: 'pretty', 85: 'pictures', 86: 'painted', 87: 'mainstream', 88: 'audiences', 89: 'charm', 90: 'romanceoz', 91: 'mess', 92: 'saw', 93: 'nasty', 94: 'surreal', 95: 'ready', 96: 'watched', 97: 'developed', 98: 'taste', 99: 'got', 100: 'accustomed', 101: 'levels', 102: 'graphic', 103: 'injustice', 104: 'crooked', 105: 'guards', 106: 'sold', 107: 'nickel', 108: 'inmates', 109: 'kill', 110: 'order', 111: 'away', 112: 'mannered', 113: 'middle', 114: 'class', 115: 'turned', 116: 'bitches', 117: 'lack', 118: 'street', 119: 'skills', 120: 'experience', 121: 'comfortable', 122: 'uncomfortable', 123: 'viewingthats', 124: 'touch', 125: 'darker', 126: 'wonderful', 127: 'little', 128: 'production', 129: 'filming', 130: 'technique', 131: 'unassuming', 132: 'oldtimebbc', 133: 'fashion', 134: 'gives', 135: 'comforting', 136: 'discomforting', 137: 'sense', 138: 'realism', 139: 'entire', 140: 'piece', 141: 'actors', 142: 'extremely', 143: 'chosen', 144: 'michael', 145: 'sheen', 146: 'polari', 147: 'voices', 148: 'pat', 149: 'truly', 150: 'seamless', 151: 'editing', 152: 'guided', 153: 'references', 154: 'williams', 155: 'diary', 156: 'entries', 157: 'worth', 158: 'terrificly', 159: 'written', 160: 'performed', 161: 'masterful', 162: 'great', 163: 'masters', 164: 'comedy', 165: 'life', 166: 'comes', 167: 'things', 168: 'fantasy', 169: 'guard', 170: 'traditional', 171: 'dream', 172: 'techniques', 173: 'remains', 174: 'solid', 175: 'disappears', 176: 'plays', 177: 'knowledge', 178: 'senses', 179: 'particularly', 180: 'concerning', 181: 'orton', 182: 'halliwell', 183: 'sets', 184: 'flat', 185: 'halliwells', 186: 'murals', 187: 'decorating', 188: 'surface', 189: 'terribly', 190: 'thought', 191: 'way', 192: 'spend', 193: 'time', 194: 'hot', 195: 'summer', 196: 'weekend', 197: 'sitting', 198: 'air', 199: 'conditioned', 200: 'theater', 201: 'lighthearted', 202: 'plot', 203: 'simplistic', 204: 'dialogue', 205: 'witty', 206: 'characters', 207: 'likable', 208: 'bread', 209: 'suspected', 210: 'serial', 211: 'killer', 212: 'disappointed', 213: 'realize', 214: 'match', 215: 'point', 216: '2', 217: 'risk', 218: 'addiction', 219: 'proof', 220: 'woody', 221: 'allen', 222: 'fully', 223: 'control', 224: 'style', 225: 'grown', 226: 'lovebr', 227: 'd', 228: 'laughed', 229: 'woodys', 230: 'comedies', 231: 'years', 232: 'decade', 233: 've', 234: 'impressed', 235: 'scarlet', 236: 'johanson', 237: 'managed', 238: 'tone', 239: 'sexy', 240: 'image', 241: 'jumped', 242: 'average', 243: 'spirited', 244: 'young', 245: 'womanbr', 246: 'crown', 247: 'jewel', 248: 'career', 249: 'wittier', 250: 'devil', 251: 'wears', 252: 'prada', 253: 'interesting', 254: 'superman', 255: 'friends', 256: 'basically', 257: 's', 258: 'family', 259: 'boy', 260: 'jake', 261: 'thinks', 262: 'zombie', 263: 'closet', 264: 'parents', 265: 'fighting', 266: 'timebr', 267: 'movie', 268: 'slower', 269: 'soap', 270: 'opera', 271: 'suddenly', 272: 'decides', 273: 'rambo', 274: 'zombiebr', 275: 'ok', 276: 'going', 277: 'film', 278: 'decide', 279: 'thriller', 280: 'drama', 281: 'watchable', 282: 'divorcing', 283: 'arguing', 284: 'like', 285: 'real', 286: 'totally', 287: 'ruins', 288: 'expected', 289: 'boogeyman', 290: 'similar', 291: 'instead', 292: 'meaningless', 293: 'spotsbr', 294: '3', 295: '10', 296: 'playing', 297: 'descent', 298: 'dialogs', 299: 'shots', 300: 'ignore', 301: 'petter', 302: 'matteis', 303: 'love', 304: 'money', 305: 'visually', 306: 'stunning', 307: 'watch', 308: 'mr', 309: 'mattei', 310: 'offers', 311: 'vivid', 312: 'portrait', 313: 'human', 314: 'relations', 315: 'telling', 316: 'power', 317: 'success', 318: 'people', 319: 'different', 320: 'situations', 321: 'encounter', 322: 'variation', 323: 'arthur', 324: 'schnitzlers', 325: 'play', 326: 'theme', 327: 'director', 328: 'transfers', 329: 'action', 330: 'present', 331: 'new', 332: 'york', 333: 'meet', 334: 'connect', 335: 'connected', 336: 'person', 337: 'know', 338: 'previous', 339: 'contact', 340: 'stylishly', 341: 'sophisticated', 342: 'luxurious', 343: 'look', 344: 'taken', 345: 'live', 346: 'world', 347: 'habitatbr', 348: 'gets', 349: 'souls', 350: 'picture', 351: 'stages', 352: 'loneliness', 353: 'inhabits', 354: 'big', 355: 'best', 356: 'place', 357: 'find', 358: 'sincere', 359: 'fulfillment', 360: 'discerns', 361: 'case', 362: 'encounterbr', 363: 'acting', 364: 'good', 365: 'direction', 366: 'steve', 367: 'buscemi', 368: 'rosario', 369: 'dawson', 370: 'carol', 371: 'kane', 372: 'imperioli', 373: 'adrian', 374: 'grenier', 375: 'rest', 376: 'talented', 377: 'cast', 378: 'come', 379: 'alivebr', 380: 'wish', 381: 'luck', 382: 'await', 383: 'anxiously', 384: 'work'}\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus: list[str]):\n",
    "    vocab = Counter(term for doc in corpus for term in doc)\n",
    "    word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "    idx_to_word = {idx: word for idx, (word, _) in enumerate(vocab.items())}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "word_to_idx, idx_to_word = build_vocab(cleaned_corpus[:5])\n",
    "print(word_to_idx)\n",
    "print(idx_to_word)\n",
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['reviewers', 'mentioned', 'watching'], 'reviewers'),\n",
       " (['reviewers', 'mentioned', 'watching', '1'], 'mentioned'),\n",
       " (['reviewers', 'mentioned', 'watching', '1', 'oz'], 'watching'),\n",
       " (['mentioned', 'watching', '1', 'oz', 'episode'], '1'),\n",
       " (['watching', '1', 'oz', 'episode', 'll'], 'oz')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_context_target_pairs(corpus: list[str], window_size: int = 2):\n",
    "    pairs = []\n",
    "    for document in corpus:\n",
    "        for idx, term in enumerate(document):\n",
    "            start_idx = max(idx - window_size, 0)\n",
    "            end_idx = min(idx + window_size + 1, len(document))\n",
    "            pairs.append(([document[i] for i in range(start_idx, end_idx)], term))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = create_context_target_pairs(cleaned_corpus)\n",
    "pairs[:5]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1, 2], 0),\n",
       " ([0, 1, 2, 3], 1),\n",
       " ([0, 1, 2, 3, 4], 2),\n",
       " ([1, 2, 3, 4, 5], 3),\n",
       " ([2, 3, 4, 5, 6], 4)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_pairs(pairs: list[str], word_to_idx: dict):\n",
    "    encoded_pairs = []\n",
    "    for context, target in pairs:\n",
    "        context_idx = [word_to_idx[term] for term in context]\n",
    "        target_idx = word_to_idx[target]\n",
    "        encoded_pairs.append((context_idx, target_idx))\n",
    "\n",
    "    return encoded_pairs\n",
    "\n",
    "encoded_pairs = encode_pairs(pairs, word_to_idx)\n",
    "encoded_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def convert_to_dataset(encoded_pairs: list, batch_size: int):\n",
    "    context_data, target_data = zip(*encoded_pairs)\n",
    "    context_tensors = [torch.tensor(context, dtype=torch.long) for context in context_data]\n",
    "    context_tensor = pad_sequence(context_tensors, batch_first=True)\n",
    "    target_tensor = torch.tensor(target_data, dtype=torch.long)\n",
    "    dataset = TensorDataset(context_tensor, target_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def create_dataset(corpus: list[str], batch_size: int, test_size: float = 0.2) -> tuple[DataLoader, DataLoader, dict]:\n",
    "    cleaned_corpus = clean_text(corpus)\n",
    "    word_to_idx, idx_to_word = build_vocab(cleaned_corpus)\n",
    "    pairs = create_context_target_pairs(cleaned_corpus)\n",
    "    encoded_pairs = encode_pairs(pairs, word_to_idx)\n",
    "    train_pairs, test_pairs = train_test_split(encoded_pairs, test_size=test_size, random_state=42)\n",
    "    return convert_to_dataset(train_pairs, batch_size), convert_to_dataset(test_pairs, batch_size), word_to_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Embedding(vocab_size, embedding_dim)`: This creates a lookup table for the embeddings. It takes two arguments:\n",
    "- `vocab_size`: The size of the vocabulary (number of unique words in the corpus).\n",
    "- `embedding_dim`: The dimension of the embeddings.\n",
    "\n",
    "When you pass a tensor of indices to the `Embedding` layer, it looks up the embeddings for each index and returns a tensor of shape `(batch_size, sequence_length, embedding_dim)`.\n",
    "\n",
    "in the forward pass, the mean of the embeddings is taken and then passed to the linear layer. this is necessary as mean is a good choice for cbow model because:\n",
    "- Stability: Averaging smooths out noise and produces more stable representations.\n",
    "- Invariance to Context Size: Whether you have 2 or 10 context words, you get an embedding of the same dimension and approximate magnitude.\n",
    "- Computational Efficiency: It's simple and fast to compute.\n",
    "- Theoretical Interpretation: It can be interpreted as estimating the expected context embedding for a given target word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        embedded = self.embeddings(context)\n",
    "        embedded = F.dropout(embedded, p=0.1)\n",
    "        embedded = embedded.mean(dim=1)\n",
    "        out = self.linear(embedded)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, context):\n",
    "        logits = self(context)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_idx = torch.argmax(probs, dim=1)\n",
    "        return pred_idx\n",
    "    \n",
    "    def cosine_similarity(self, word1_idx, word2_idx):\n",
    "        word1_embedding = self.embeddings(torch.tensor([word1_idx]))\n",
    "        word2_embedding = self.embeddings(torch.tensor([word2_idx]))\n",
    "        similarity = F.cosine_similarity(word1_embedding, word2_embedding)\n",
    "        return similarity.item()\n",
    "    \n",
    "    def similarity(self, word1, word2):\n",
    "        word1_idx = self.vocab[word1]\n",
    "        word2_idx = self.vocab[word2]\n",
    "        return self.cosine_similarity(word1_idx, word2_idx)\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, epochs: int, learning_rate: float):\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\", total=epochs):\n",
    "        total_loss = 0\n",
    "        test_loss = 0\n",
    "        model.train()\n",
    "        for X_train, y_train in train_loader:\n",
    "            y_logits = model(X_train) # forward pass\n",
    "            loss = loss_function(y_logits, y_train) # compute loss\n",
    "\n",
    "            optimizer.zero_grad() # set gradients to zero\n",
    "            loss.backward() # backward pass to compute gradients that are used to update the weights\n",
    "            optimizer.step() # update the weights\n",
    "            total_loss += loss.item() # add the loss to the total loss\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X_test, y_test in test_loader:\n",
    "                y_logits = model(X_test)\n",
    "                test_loss_fn = loss_function(y_logits, y_test)\n",
    "                test_loss += test_loss_fn.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} | Train Loss: {total_loss} | Test Loss: {test_loss}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 1/100 [00:12<21:20, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 22541.599172592163 | Test Loss: 5439.917895317078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 11/100 [02:23<19:01, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 13056.894749641418 | Test Loss: 6550.245553016663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 21/100 [04:07<13:43, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 11774.263401031494 | Test Loss: 7210.885009765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 31/100 [05:55<11:57, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 11182.8458943367 | Test Loss: 7676.044419765472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 41/100 [07:32<09:31,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 | Train Loss: 10885.312699317932 | Test Loss: 8016.459345817566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 51/100 [09:18<08:41, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | Train Loss: 10700.528440237045 | Test Loss: 8272.693334579468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 61/100 [11:02<06:47, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 | Train Loss: 10564.257967948914 | Test Loss: 8505.10803604126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 71/100 [12:56<05:57, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 | Train Loss: 10455.07897400856 | Test Loss: 8650.00625038147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 81/100 [15:01<03:43, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 | Train Loss: 10384.208398103714 | Test Loss: 8808.534088134766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 91/100 [17:09<01:54, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 | Train Loss: 10321.743609905243 | Test Loss: 8911.629905223846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [19:08<00:00, 11.49s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, vocab = create_dataset(corpus[:1000], 32)\n",
    "print(len(vocab))\n",
    "embedding_dim = 12\n",
    "model = CBOWModel(vocab=vocab, embedding_dim=embedding_dim)\n",
    "losses = train_model(model, train_loader, test_loader, epochs=100, learning_rate=0.01)\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"cbow_model.pt\")\n",
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/9mc5gz114tv5861ps8xb77nr0000gn/T/ipykernel_5244/2396995196.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(\"cbow_model.pt\")\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "import torch\n",
    "# Load the model state dictionary\n",
    "model_state_dict = torch.load(\"cbow_model.pt\")\n",
    "\n",
    "# Load the model directly from the state dictionary\n",
    "model = model_state_dict\n",
    "\n",
    "# No need to create a new instance and load state dictionary separately\n",
    "# since the saved model contains all the necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99
         ],
         "xaxis": "x",
         "y": [
          22541.599172592163,
          19093.984898090363,
          17320.333058834076,
          16141.992027282715,
          15316.870606899261,
          14725.597985744476,
          14254.604752540588,
          13884.08397102356,
          13567.933984994888,
          13293.871767044067,
          13056.894749641418,
          12867.276861429214,
          12678.40651011467,
          12541.648206233978,
          12370.461008548737,
          12246.735395908356,
          12144.648064136505,
          12036.842065811157,
          11930.18250632286,
          11836.17512845993,
          11774.263401031494,
          11694.547816753387,
          11609.576929330826,
          11566.797608613968,
          11491.53314089775,
          11439.495023012161,
          11399.40960097313,
          11338.170349121094,
          11296.291029214859,
          11241.793305158615,
          11182.8458943367,
          11173.246168613434,
          11125.784052610397,
          11092.29272532463,
          11073.290061950684,
          11021.349274635315,
          11007.331273317337,
          10974.55573797226,
          10947.598940849304,
          10904.901110649109,
          10885.312699317932,
          10870.055027008057,
          10848.987998485565,
          10824.998927593231,
          10808.540892839432,
          10797.717164754868,
          10762.837553977966,
          10737.144497871399,
          10714.822422504425,
          10712.986698150635,
          10700.528440237045,
          10675.066133975983,
          10658.47518157959,
          10639.395395755768,
          10624.51338005066,
          10619.9240026474,
          10606.83561205864,
          10612.40843296051,
          10590.042355775833,
          10563.27967619896,
          10564.257967948914,
          10551.066346645355,
          10519.008152484894,
          10536.869199752808,
          10505.83925819397,
          10504.27700972557,
          10505.66379404068,
          10490.387787342072,
          10470.66353392601,
          10471.415085554123,
          10455.07897400856,
          10474.06832742691,
          10442.406576156616,
          10446.331690073013,
          10418.868655443192,
          10428.690821886063,
          10409.502593755722,
          10390.101939201355,
          10398.13772559166,
          10400.352364301682,
          10384.208398103714,
          10377.925644040108,
          10384.195572853088,
          10358.883752346039,
          10375.304719686508,
          10346.703163385391,
          10339.138641119003,
          10339.290333509445,
          10331.967076301575,
          10325.296192407608,
          10321.743609905243,
          10333.344325065613,
          10300.68135380745,
          10321.85531592369,
          10285.702659130096,
          10272.532291650772,
          10276.083911657333,
          10292.430754184723,
          10303.349501609802,
          10293.434086799622
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss Curve"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def loss_curve(losses: list[float]):\n",
    "    fig = px.line(x=range(len(losses)), y=losses, title='Loss Curve')\n",
    "    fig.show()\n",
    "\n",
    "loss_curve(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norms: [ 1.2366066  8.320855   7.452545  ... 19.6043    17.017044  25.644018 ]\n",
      "embeddings_normalized: [[ 0.10190571  0.10056304  0.10401462 ...  0.05363297 -0.18942468\n",
      "  -0.36074483]\n",
      " [ 0.20849703 -0.03250107 -0.6173669  ... -0.01910633  0.18679707\n",
      "  -0.42084056]\n",
      " [ 0.3873951  -0.11529763 -0.0641541  ...  0.18682756  0.23886132\n",
      "   0.01504575]\n",
      " ...\n",
      " [-0.5978849   0.03820896 -0.48145422 ...  0.20628786 -0.3227963\n",
      "  -0.367883  ]\n",
      " [ 0.21028958  0.07459436 -0.36274526 ...  0.34811535 -0.5412496\n",
      "  -0.53862935]\n",
      " [ 0.14371923  0.27519608 -0.08739192 ...  0.32810763 -0.06639595\n",
      "   0.22749989]]\n",
      "A: man, B: woman, C: boy, D: girl\n",
      "785 619 259 1062\n",
      "X: [ 0.12723988 -0.37427738 -0.1376323  -0.49823642  0.531686    0.2920439\n",
      "  0.32796383 -0.74903715 -0.1466363   0.33375967 -0.28666532  0.19823392]\n",
      "similarity: 0.6867948770523071\n",
      "A: king, B: queen, C: prince, D: princess\n",
      "3299 5958 15078 4692\n",
      "X: [ 0.4116267   0.62182826  0.90117663 -0.08440593  0.46371832  0.37712312\n",
      "  0.5110897  -0.40262944 -0.9548292   0.47567308  0.5880734   0.587147  ]\n",
      "similarity: -0.32253462076187134\n",
      "A: father, B: mother, C: brother, D: sister\n",
      "1523 1533 591 933\n",
      "X: [ 0.44736856 -0.28746247 -0.24236059 -0.3478901   0.25738698  0.0465762\n",
      "  0.27804345  0.04370531 -0.17528196  0.01747806  0.57726836  0.4462358 ]\n",
      "similarity: -0.4180932641029358\n",
      "A: doctor, B: hospital, C: teacher, D: school\n",
      "2331 2880 8281 799\n",
      "X: [-0.11884682 -0.19557717  0.00783566 -0.09702557  0.1602934   0.14231104\n",
      " -0.8502205   0.8171216   0.3557228  -1.1043193   0.78862906  0.15137495]\n",
      "similarity: 0.4587603211402893\n",
      "A: apple, B: fruit, C: carrot, D: vegetable\n",
      "Skipping analogy: apple : fruit :: carrot : vegetable (word not in vocabulary)\n",
      "A: dog, B: bark, C: cat, D: meow\n",
      "Skipping analogy: dog : bark :: cat : meow (word not in vocabulary)\n",
      "A: book, B: read, C: movie, D: watch\n",
      "2308 468 267 307\n",
      "X: [ 0.04350039  0.1116337  -0.05822538  0.11660552 -0.0757354   0.17669715\n",
      "  0.31517968  0.3455194   0.19971989  0.7622988  -0.15464482 -0.64589846]\n",
      "similarity: 0.8197306990623474\n",
      "A: water, B: drink, C: food, D: eat\n",
      "456 5904 6046 3687\n",
      "X: [-0.64318144  0.07482833  0.95998293  0.02182971 -0.6491381   0.39326385\n",
      " -0.27967975  0.40126282 -0.6188184   0.8107219  -0.513765   -0.48609877]\n",
      "similarity: 0.5753042697906494\n",
      "A: car, B: drive, C: bicycle, D: ride\n",
      "Skipping analogy: car : drive :: bicycle : ride (word not in vocabulary)\n",
      "A: Paris, B: France, C: Berlin, D: Germany\n",
      "Skipping analogy: Paris : France :: Berlin : Germany (word not in vocabulary)\n",
      "A: walk, B: walked, C: run, D: ran\n",
      "1343 5843 1150 2902\n",
      "X: [ 1.75205708e-01 -2.01759398e-01 -3.51704240e-01 -3.36341560e-04\n",
      " -5.66779494e-01 -1.20126486e-01  3.49774390e-01  6.30197406e-01\n",
      " -5.57544678e-02  5.15345037e-01  5.05654812e-01 -1.74842834e-01]\n",
      "similarity: -0.3075921833515167\n",
      "A: eat, B: ate, C: see, D: saw\n",
      "Skipping analogy: eat : ate :: see : saw (word not in vocabulary)\n",
      "A: good, B: better, C: bad, D: worse\n",
      "364 589 502 1146\n",
      "X: [-0.09669909  0.61945456  0.15843955 -0.00171602 -0.72415566 -0.18120295\n",
      " -0.3968531   0.5030426  -0.55853057 -0.6893967   0.01649368 -1.2082033 ]\n",
      "similarity: 0.9981387853622437\n",
      "A: big, B: bigger, C: small, D: smaller\n",
      "354 2496 1675 6932\n",
      "X: [-0.2420751   0.8668835  -0.50593483 -0.5794538  -0.20018537 -0.34186316\n",
      " -0.2299056   0.3363955  -0.52452374 -0.03571755 -0.06064609 -0.01396968]\n",
      "similarity: -0.3360104560852051\n",
      "A: child, B: children, C: man, D: men\n",
      "2283 419 785 600\n",
      "X: [-0.5543773  -0.20142825  0.1518288  -0.5125009  -1.0481312   0.52826834\n",
      " -0.16835602 -0.706005   -0.25787988 -0.6906574  -0.16588539 -0.9001726 ]\n",
      "similarity: 0.1380193829536438\n",
      "A: go, B: going, C: come, D: coming\n",
      "Skipping analogy: go : going :: come : coming (word not in vocabulary)\n",
      "A: happy, B: happier, C: sad, D: sadder\n",
      "Skipping analogy: happy : happier :: sad : sadder (word not in vocabulary)\n",
      "A: think, B: thought, C: feel, D: felt\n",
      "945 190 850 521\n",
      "X: [ 0.39485008 -0.3215248  -0.03417964 -0.2965359  -0.6775986  -0.75362176\n",
      " -0.79562306  0.5777918  -0.09989103 -0.06205016 -0.29650056  0.31738868]\n",
      "similarity: 0.49388647079467773\n",
      "A: write, B: written, C: speak, D: spoken\n",
      "772 159 1404 4812\n",
      "X: [-0.9620645   0.5523499  -0.59330314 -0.3102739   0.62439466 -0.5154108\n",
      " -0.3185437  -0.27316064  0.20544994 -0.15737352  0.10992822  1.0389304 ]\n",
      "similarity: 0.05416102707386017\n",
      "A: play, B: played, C: work, D: worked\n",
      "325 542 384 1293\n",
      "X: [-0.20934886  0.02841704  0.00755148 -0.3679249   1.4881186  -0.3042209\n",
      "  0.13160785 -0.5354508   0.37254518 -0.42616582 -0.25699437  0.03572427]\n",
      "similarity: 0.3124997019767761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.6923076923076925, 0.24254346237732813)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogies = [\n",
    "    (\"man\", \"woman\", \"boy\", \"girl\"),\n",
    "    (\"king\", \"queen\", \"prince\", \"princess\"),\n",
    "    (\"father\", \"mother\", \"brother\", \"sister\"),\n",
    "    (\"doctor\", \"hospital\", \"teacher\", \"school\"),\n",
    "    (\"apple\", \"fruit\", \"carrot\", \"vegetable\"),\n",
    "    (\"dog\", \"bark\", \"cat\", \"meow\"),\n",
    "    (\"book\", \"read\", \"movie\", \"watch\"),\n",
    "    (\"water\", \"drink\", \"food\", \"eat\"),\n",
    "    (\"car\", \"drive\", \"bicycle\", \"ride\"),\n",
    "    (\"Paris\", \"France\", \"Berlin\", \"Germany\"),\n",
    "    (\"walk\", \"walked\", \"run\", \"ran\"),\n",
    "    (\"eat\", \"ate\", \"see\", \"saw\"),\n",
    "    (\"good\", \"better\", \"bad\", \"worse\"),\n",
    "    (\"big\", \"bigger\", \"small\", \"smaller\"),\n",
    "    (\"child\", \"children\", \"man\", \"men\"),\n",
    "    (\"go\", \"going\", \"come\", \"coming\"),\n",
    "    (\"happy\", \"happier\", \"sad\", \"sadder\"),\n",
    "    (\"think\", \"thought\", \"feel\", \"felt\"),\n",
    "    (\"write\", \"written\", \"speak\", \"spoken\"),\n",
    "    (\"play\", \"played\", \"work\", \"worked\")\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_analogy_accuracy(embeddings, word_to_idx, analogies):\n",
    "    \"\"\"\n",
    "    Compute the word analogy accuracy using cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: numpy array of shape (vocab_size, embedding_dim), the word embeddings from the CBOW model\n",
    "    - word_to_idx: dict mapping words to their indices in the embeddings matrix\n",
    "    - analogies: list of tuples (A, B, C, D) representing analogies \"A is to B as C is to D\"\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: float, percentage of correct predictions\n",
    "    \"\"\"\n",
    "    # Normalize embeddings to unit length for cosine similarity\n",
    "    norms = np.linalg.norm(embeddings, axis=1)\n",
    "    print(f\"norms: {norms}\")\n",
    "    embeddings_normalized = embeddings / norms[:, np.newaxis]\n",
    "    print(f\"embeddings_normalized: {embeddings_normalized}\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    avg_similarity = 0\n",
    "    for A, B, C, D in analogies:\n",
    "        print(f\"A: {A}, B: {B}, C: {C}, D: {D}\")\n",
    "        # Skip analogy if any word is not in the vocabulary\n",
    "        if A not in word_to_idx or B not in word_to_idx or C not in word_to_idx or D not in word_to_idx:\n",
    "            print(f\"Skipping analogy: {A} : {B} :: {C} : {D} (word not in vocabulary)\")\n",
    "            continue\n",
    "        \n",
    "        # Get indices of words A, B, C, D\n",
    "        idx_A = word_to_idx[A]\n",
    "        idx_B = word_to_idx[B]\n",
    "        idx_C = word_to_idx[C]\n",
    "        idx_D = word_to_idx[D]\n",
    "        print(idx_A, idx_B, idx_C, idx_D)\n",
    "        \n",
    "        # Compute the analogy vector X = vector(B) - vector(A) + vector(C)\n",
    "        X = embeddings_normalized[idx_B] - embeddings_normalized[idx_A] + embeddings_normalized[idx_C]\n",
    "        print(f\"X: {X}\")\n",
    "        \n",
    "        similarity_threshold = 0.90\n",
    "        # similarity between X and D\n",
    "        similarity = np.dot(X, embeddings_normalized[idx_D])\n",
    "        print(f\"similarity: {similarity}\")\n",
    "        avg_similarity += similarity\n",
    "        if similarity >= similarity_threshold:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    \n",
    "    # Handle case where no valid analogies were processed\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute accuracy as percentage\n",
    "    accuracy = correct / total * 100\n",
    "    avg_similarity /= total\n",
    "    return accuracy, avg_similarity\n",
    "\n",
    "\n",
    "embeddings = model.embeddings.weight.data.numpy()\n",
    "acc, avg_similarity = compute_analogy_accuracy(embeddings, vocab, analogies)\n",
    "acc, avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18516, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
