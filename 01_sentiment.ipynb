{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import  Counter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 80488\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynasent_r1  = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r1.all', trust_remote_code=True)\n",
    "dynasent_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 14021\n",
      "\tneutral: 45076\n",
      "\tpositive: 21391\n",
      "validation\n",
      "\tnegative: 1200\n",
      "\tneutral: 1200\n",
      "\tpositive: 1200\n"
     ]
    }
   ],
   "source": [
    "def print_label_dist(dataset, label_name=\"gold_label\", split_names=(\"train\", \"validation\")):\n",
    "    for split_name in split_names:\n",
    "        print(split_name)\n",
    "        dist = sorted(Counter(dataset[split_name][label_name]).items())\n",
    "        for k, v in dist:\n",
    "            print(f\"\\t{k:>4s}: {v}\")\n",
    "\n",
    "print_label_dist(dynasent_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dynasent_r2 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r2.all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'r2-0000042',\n",
       " 'hit_ids': ['y22532', 'y22740'],\n",
       " 'sentence': 'He kindly walked us through the menu through the specialties and the wine list that we did not want.',\n",
       " 'sentence_author': 'w148',\n",
       " 'has_prompt': True,\n",
       " 'prompt_data': {'indices_into_review_text': [453, 525],\n",
       "  'review_rating': 5,\n",
       "  'prompt_sentence': 'He walked us through the menu through the specialties and the wine list.',\n",
       "  'review_id': '7joEfaGr4aC0OrR9I7CJ8Q'},\n",
       " 'model_1_label': 'positive',\n",
       " 'model_1_probs': {'negative': 0.15193994343280792,\n",
       "  'positive': 0.7109395265579224,\n",
       "  'neutral': 0.1371205449104309},\n",
       " 'text_id': 'r2-0000042',\n",
       " 'label_distribution': {'positive': [],\n",
       "  'negative': ['w516', 'w199', 'w294'],\n",
       "  'neutral': ['w544'],\n",
       "  'mixed': ['w527']},\n",
       " 'gold_label': 'negative',\n",
       " 'metadata': {'split': 'train',\n",
       "  'round': 2,\n",
       "  'subset': 'all',\n",
       "  'model_in_the_loop': 'RoBERTa'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynasent_r2['train'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 4579\n",
      "\tneutral: 2448\n",
      "\tpositive: 6038\n",
      "validation\n",
      "\tnegative: 240\n",
      "\tneutral: 240\n",
      "\tpositive: 240\n"
     ]
    }
   ],
   "source": [
    "print_label_dist(dynasent_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "sst = load_dataset(\"SetFit/sst5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 2218\n",
      "\tneutral: 1624\n",
      "\tpositive: 2322\n",
      "\tvery negative: 1092\n",
      "\tvery positive: 1288\n",
      "validation\n",
      "\tnegative: 289\n",
      "\tneutral: 229\n",
      "\tpositive: 279\n",
      "\tvery negative: 139\n",
      "\tvery positive: 165\n"
     ]
    }
   ],
   "source": [
    "print_label_dist(sst, label_name='label_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"a well-intentioned effort that 's still too burdened by the actor 's offbeat sensibilities for the earnest emotional core to emerge with any degree of accessibility .\",\n",
       " 'label': 1,\n",
       " 'label_text': 'negative'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst[\"train\"][35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reformart the SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 3310\n",
      "\tneutral: 1624\n",
      "\tpositive: 3610\n",
      "validation\n",
      "\tnegative: 428\n",
      "\tneutral: 229\n",
      "\tpositive: 444\n"
     ]
    }
   ],
   "source": [
    "def conver_sst_label(label):\n",
    "    return label.split(\" \")[-1]\n",
    "\n",
    "for split_name in (\"train\", \"test\", \"validation\"):\n",
    "    dist = [conver_sst_label(s) for s in sst[split_name]['label_text']]\n",
    "    sst[split_name] = sst[split_name].add_column(\"gold_label\", dist)\n",
    "    sst[split_name] = sst[split_name].add_column(\"sentence\", sst[split_name][\"text\"])\n",
    "\n",
    "print_label_dist(sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hello!': 1, 'i': 1, 'love': 1, 'machine': 1, 'learning!': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigrams_phi(sentence: str):\n",
    "    return Counter(sentence.lower().split(\" \"))\n",
    "\n",
    "uni = unigrams_phi(\"Hello! I love Machine Learning!\")\n",
    "uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c\n",
       "0  1.0  1.0  0.0\n",
       "1  0.0  1.0  2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = [\n",
    "    {'a': 1, 'b': 1},\n",
    "    {'b': 1, 'c': 2}\n",
    "]\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "X_train = vec.fit_transform(train_feats)\n",
    "\n",
    "df = pd.DataFrame(X_train, columns=vec.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c\n",
       "0  2.0  0.0  1.0\n",
       "1  4.0  2.0  0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats = [\n",
    "    {'a': 2, 'c': 1},\n",
    "    {'a': 4, 'b': 2, 'd': 1}\n",
    "]\n",
    "\n",
    "X_test = vec.transform(test_feats)\n",
    "df = pd.DataFrame(X_test, columns=vec.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common mistake with DictVectorizer is calling fit_transform on test examples. This will wipe out the existing representation scheme, replacing it with one that matches the test examples. That will happen silently, but then you'll find that the new representations are incompatible with the model you fit. This is likely to manifest itself as a ValueError relating to feature counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tweetgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Let': 1, 'that': 1, 'SINK': 1, 'in': 1, ':)': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tweetgrams_phi(sentence: str, **kwargs):\n",
    "    tk = TweetTokenizer(kwargs)\n",
    "    tokens = tk.tokenize(sentence)\n",
    "    return Counter(tokens)\n",
    "\n",
    "tweet = \"Let that SINK in :)\"\n",
    "\n",
    "grams = tweetgrams_phi(tweet, preserve_case=False)\n",
    "grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tweetgrams_phi(func):\n",
    "    examples = [\n",
    "        (\n",
    "            \"Here's an example with an emoticon :)\", \n",
    "            Counter({'an': 2, \"Here's\": 1, 'example': 1, 'with': 1, 'emoticon': 1, ':)': 1})\n",
    "        ),\n",
    "        (\n",
    "            \"The URL is https://pytorch.org!\", \n",
    "            Counter({'The': 1, 'URL': 1, 'is': 1, 'https://pytorch.org': 1, '!': 1})\n",
    "        )\n",
    "    ]\n",
    "    errcount = 0\n",
    "    for ex, expected in examples:\n",
    "        result = func(ex, preserve_case=True)\n",
    "        if result != expected:\n",
    "            errcount += 1\n",
    "            print(f\"Error for `{func.__name__}`: For input {ex}, \"\n",
    "                  f\"expected {expected} but got {result}\")\n",
    "    caps_ex = \"CAPS\"\n",
    "    caps_result = func(caps_ex, preserve_case=False)\n",
    "    caps_expected = Counter({\"CAPS\": 1})\n",
    "    if caps_result != caps_expected:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: For input {caps_ex}, \"\n",
    "              f\"expected {caps_expected} but got {caps_result}\")\n",
    "    if errcount == 0:\n",
    "        print(f\"All tests passed for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed for `tweetgrams_phi`\n"
     ]
    }
   ],
   "source": [
    "test_tweetgrams_phi(tweetgrams_phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), DictVectorizer())"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_linear_model(model, featfunc, train_dataset):\n",
    "    \"\"\"Train an sklearn classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn classifier model\n",
    "    featfunc : func\n",
    "        Maps strings to Counter instances\n",
    "    train_dataset: dict\n",
    "        Must have a key \"sentence\" containing strings that `featfunc`\n",
    "        will process, and a key \"gold_label\" giving labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        * A trained version of `model`\n",
    "        * A fitted `vectorizer` for the train set\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feats = [dict(featfunc(s)) for s in train_dataset['sentence']]\n",
    "    vec = DictVectorizer()\n",
    "    X_train = vec.fit_transform(feats)\n",
    "    model.fit(X_train, train_dataset['gold_label'])\n",
    "    return model, vec\n",
    "\n",
    "\n",
    "train_dataset = {\n",
    "    'sentence': ['A A', 'A B', 'B B', 'B A', 'B'],\n",
    "    'gold_label': [0, 1, 0, 1, 1]\n",
    "}\n",
    "def featfunc(s):\n",
    "    return Counter(s.split())\n",
    "\n",
    "model = LogisticRegression()\n",
    "train_linear_model(model, featfunc, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `train_linear_model`\n"
     ]
    }
   ],
   "source": [
    "def test_train_linear_model(func):\n",
    "    train_dataset = {\n",
    "        'sentence': ['A A', 'A B', 'B B', 'B A', 'B'],\n",
    "        'gold_label': [0, 1, 0, 1, 1]}\n",
    "    def featfunc(s):\n",
    "        return Counter(s.split())\n",
    "    model = LogisticRegression()\n",
    "    result = func(model, featfunc, train_dataset)\n",
    "    if not isinstance(result, tuple) or len(result) != 2:\n",
    "        print(f\"Error for `{func.__name__}`: Incorrect return type\")\n",
    "        return\n",
    "    model, vectorizer = result\n",
    "    if not hasattr(vectorizer, 'vocabulary_'):\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Second return value is not a trained vectorizer\")\n",
    "        return\n",
    "    if not hasattr(model, 'classes_'):\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"First return value is not a trained classifier\")\n",
    "        return\n",
    "    print(f\"No errors found for `{func.__name__}`\")\n",
    "\n",
    "_ =  test_train_linear_model(train_linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model on dynasent R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_unigrams, vec_unigrams = train_linear_model(\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    unigrams_phi, dynasent_r1['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_linear_model(model, featfunc, vectorizer, assess_dataset):\n",
    "    \"\"\"Assess a trained sklearn model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: trained sklearn model\n",
    "    featfunc : func\n",
    "        Maps strings to count dicts\n",
    "    vectorizer : fitted DictVectorizer\n",
    "    assess_dataset: dict\n",
    "        Must have a key \"sentence\" containing strings that `featfunc`\n",
    "        will process, and a key \"gold_label\" giving labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A classification report (multiline string)\n",
    "\n",
    "    \"\"\"\n",
    "    feats = [featfunc(s) for s in assess_dataset['sentence']]\n",
    "    X_test = vectorizer.transform(feats)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    return classification_report(assess_dataset['gold_label'], preds, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `assess_linear_model`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishwasgowda/code/nlu/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/vishwasgowda/code/nlu/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/vishwasgowda/code/nlu/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def test_assess_linear_model(assessfunc, trainfunc):\n",
    "    train_dataset = {\n",
    "        'sentence': ['A A', 'A B', 'B B', 'B A', 'A', 'B'],\n",
    "        'gold_label': [0, 1, 0, 1, 0, 1]}\n",
    "    assess_dataset = {\n",
    "        'sentence': ['A C', 'B A'],\n",
    "        'gold_label': [0, 1]}\n",
    "    def featfunc(s):\n",
    "        return Counter(s.split())\n",
    "    model = LogisticRegression()\n",
    "    model, vectorizer = trainfunc(model, featfunc, train_dataset)\n",
    "    result = assessfunc(model, featfunc, vectorizer, assess_dataset)\n",
    "    errcount = 0\n",
    "    if len(vectorizer.vocabulary_) != 2:\n",
    "        print(f\"Error for `{assessfunc.__name__}`: Unexpected feature count\")\n",
    "        errcount += 1\n",
    "    if 'weighted avg' not in result:\n",
    "        print(f\"Error for `{assessfunc.__name__}`: Unexpected return value\")\n",
    "        errcount += 1\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors found for `{assessfunc.__name__}`\")\n",
    "\n",
    "test_assess_linear_model(assess_linear_model, train_linear_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.759     0.364     0.492      1200\n",
      "     neutral      0.523     0.890     0.659      1200\n",
      "    positive      0.699     0.572     0.629      1200\n",
      "\n",
      "    accuracy                          0.609      3600\n",
      "   macro avg      0.660     0.609     0.593      3600\n",
      "weighted avg      0.660     0.609     0.593      3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = assess_linear_model(\n",
    "    lr_unigrams,\n",
    "    unigrams_phi,\n",
    "    vec_unigrams,\n",
    "    dynasent_r1['validation'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Transformer fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_name = \"prajjwal1/bert-mini\"\n",
    "\n",
    "bert = AutoModel.from_pretrained(weights_name)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(weights_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert', 'knows', 's', '##nu', '##ffle', '##up', '##ag', '##us']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"Bert knows Snuffleupagus\"\n",
    "bert_tokenizer.tokenize(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode() is used for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 14324, 4282, 1055, 11231, 18142, 6279, 8490, 2271, 102]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_ids = bert_tokenizer.encode(example_text, add_special_tokens=True)\n",
    "ex_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    reps = bert(torch.tensor([ex_ids]))\n",
    "\n",
    "reps.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `last_hidden_state` hidden state is the sequence of final output states from the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3763, -0.3209,  0.8817,  0.4568, -1.0314])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    reps = bert(torch.tensor([ex_ids]))\n",
    "    print(reps.last_hidden_state[0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1793, -0.8994,  0.9695,  0.9130, -0.7129])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Mask the last 5 tokens:\n",
    "    am = torch.tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
    "    maskreps = bert(torch.tensor([ex_ids]), attention_mask=am)\n",
    "    print(maskreps.last_hidden_state[0][0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Batch tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_token_ids(batch: list[str], tokenizer):\n",
    "   res = tokenizer.batch_encode_plus(\n",
    "      batch,\n",
    "      max_length=512,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors=\"pt\"\n",
    "   )\n",
    "      \n",
    "   return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_batch_token_ids(func):\n",
    "    examples = [\n",
    "        \"Bert knows Snuffleupagus\",\n",
    "        \"ELMo knew Bert.\",\n",
    "        \"Buffalo \" * 520\n",
    "    ]\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-mini\")\n",
    "    result = func(examples, test_tokenizer)\n",
    "    errcount = 0\n",
    "    if 'attention_mask' not in result:\n",
    "        errcount += 1  \n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Attention mask was not returned\")\n",
    "    ids = result['input_ids']\n",
    "    if not isinstance(ids, torch.Tensor):\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Return values are not tensors\")\n",
    "    if ids.shape[1] != 512:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Expected sequence length 512; got {ids.shape[1]}\")\n",
    "    if ids[0][0] != bert_tokenizer.cls_token_id:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Special tokens were not added\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors found for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `get_batch_token_ids`\n"
     ]
    }
   ],
   "source": [
    "test_get_batch_token_ids(get_batch_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Contextual representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reps(dataset, model, tokenizer, batchsize=20):\n",
    "    \"\"\"Represent each example in `dataset` with the final hidden state \n",
    "    above the [CLS] token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list of str\n",
    "    model : BertModel\n",
    "    tokenizer : BertTokenizerFast\n",
    "    batchsize : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor with shape `(n_examples, dim)` where `dim` is the\n",
    "    dimensionality of the representations for `model`\n",
    "\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        pass\n",
    "        # Iterate over `dataset` in batches:\n",
    "        ##### YOUR CODE HERE\n",
    "\n",
    "        for i in range(0, len(dataset), batchsize):\n",
    "            batch = dataset[i: i+batchsize]\n",
    "\n",
    "            token_ids = get_batch_token_ids(batch, tokenizer)\n",
    "            reps = model(token_ids['input_ids'], attention_mask=token_ids['attention_mask'])\n",
    "\n",
    "            cls_reps = reps.last_hidden_state[:, 0, :]\n",
    "            data.append(cls_reps)\n",
    "\n",
    "        \n",
    "        return torch.cat(data, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [\"The cat slept.\", \"The bird chirped.\"]\n",
    "test_model = AutoModel.from_pretrained(weights_name)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(weights_name)\n",
    "result = get_reps(sents, test_model, test_tokenizer, batchsize=2)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reason why only vectors of the CLS tokens are used for BERT models: the CLS token is the first token in the sequence, and it is used to represent the entire sequence. The CLS token is used for classification tasks, and the final hidden state of the CLS token is used as the aggregate sequence representation for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `get_reps`\n"
     ]
    }
   ],
   "source": [
    "def test_get_reps(func):\n",
    "    examples = [\"The cat slept.\", \"The bird chirped.\"] * 20\n",
    "    weights_name = \"prajjwal1/bert-mini\"\n",
    "    test_model = AutoModel.from_pretrained(weights_name)\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(weights_name)\n",
    "    result = func(examples, test_model, test_tokenizer, batchsize=2)\n",
    "    errcount = 0\n",
    "    if result.shape != (40, 256):\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Expected shape {(40, 256)}, got {result.shape}\")\n",
    "    if round(result[0][0].item(), 2) != -0.64:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Representations seem to be incorrect\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors found for `{func.__name__}`\")\n",
    "\n",
    "test_get_reps(get_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_classes: int,\n",
    "        hidden_activation,\n",
    "        weights_name=\"prajjwal1/bert-mini\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "            This module loads a Transformer based on  `weights_name`,\n",
    "            puts it in train mode, add a dense layer with activation\n",
    "            function give by `hidden_activation`, and puts a classifier\n",
    "            layer on top of that as the final output. The output of\n",
    "            the dense layer should have the same dimensionality as the\n",
    "            model input.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            n_classes : int\n",
    "                Number of classes for the output layer\n",
    "            hidden_activation : torch activation function\n",
    "                e.g., nn.Tanh()\n",
    "            weights_name : str\n",
    "                Name of pretrained model to load from Hugging Face\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.weights_name = weights_name\n",
    "        self.bert = AutoModel.from_pretrained(self.weights_name)\n",
    "        self.bert.train()\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, indices, mask):\n",
    "        \"\"\"\n",
    "        Process `indices` with `mask` by feeding these arguments\n",
    "        to `self.bert` and then feeding the initial hidden state\n",
    "        in `last_hidden_state` to `self.classifier_layer`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : tensor.LongTensor of shape (n_batch, k)\n",
    "            Indices into the `self.bert` embedding layer. `n_batch` is\n",
    "            the number of examples and `k` is the sequence length for\n",
    "            this batch\n",
    "        mask : tensor.LongTensor of shape (n_batch, d)\n",
    "            Binary vector indicating which values should be masked.\n",
    "            `n_batch` is the number of examples and `k` is the\n",
    "            sequence length for this batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor.FloatTensor\n",
    "            Predicted values, shape `(n_batch, self.n_classes)`\n",
    "\n",
    "        \"\"\"\n",
    "        bert_output = self.bert(input_ids=indices, attention_mask=mask)\n",
    "        cls_reps = bert_output.last_hidden_state[:, 0, :]\n",
    "        predictions = self.classifier_layer(cls_reps)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 3])\n",
      "tensor([[ 0.3307, -0.3084,  0.0894],\n",
      "        [ 0.1298, -0.1202, -0.1604]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bert_module = BertClassifierModule(n_classes=3, hidden_activation=nn.Tanh())\n",
    "\n",
    "ids = get_batch_token_ids(dynasent_r1['train']['sentence'][:2], bert_tokenizer)\n",
    "\n",
    "result = bert_module(ids['input_ids'], ids['attention_mask'])\n",
    "\n",
    "print(f\"Shape: {result.shape}\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `BertClassifierModule`\n"
     ]
    }
   ],
   "source": [
    "def test_bert_classifier_module(moduleclass): \n",
    "    expected_out = 5\n",
    "    expected_hidden = 256\n",
    "    expected_activation = nn.ReLU()\n",
    "    mod = moduleclass(expected_out, expected_activation)\n",
    "    errcount = 0\n",
    "\n",
    "    # Basic layer structure:\n",
    "    if not hasattr(mod, \"classifier_layer\") or mod.classifier_layer is None:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
    "              f\"Missing attribute `classifier_layer`\")\n",
    "        return \n",
    "    for i in range(3):\n",
    "        try:\n",
    "            bert_module.classifier_layer[i]\n",
    "        except IndexError:\n",
    "            errcount += 1\n",
    "            print(f\"Error for `{moduleclass.__name__}`: \"\n",
    "                  f\"`classifier_layer` is not an `nn.Sequential` \"\n",
    "                  f\"and/or does not have the right structure\")\n",
    "    # Correct first layer dimensionality:\n",
    "    result_hidden = mod.classifier_layer[0].out_features\n",
    "    if result_hidden != expected_hidden:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
    "              f\"Expected `classifier_layer` hidden dim {expected_hidden}, \"\n",
    "              f\"got {result_hidden}\") \n",
    "    # Correct activation:\n",
    "    result_activation = mod.classifier_layer[1].__class__.__name__\n",
    "    if result_activation != expected_activation.__class__.__name__:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
    "              f\"Incorrect hidden activation\")\n",
    "    # Correct output dimensionality:\n",
    "    result_out = mod.classifier_layer[2].out_features\n",
    "    if result_out != expected_out:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
    "              f\"Expected `classifier_layer` out dim {expected_out}, \"\n",
    "              f\"got {result_out}\")\n",
    "    # forward method:\n",
    "    ids = get_batch_token_ids([\"A B C\", \"A B\"], bert_tokenizer)\n",
    "    result = mod(ids['input_ids'], ids['attention_mask'])\n",
    "    if result.shape != (2, 5):\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
    "              f\"Expected output shape {(2, 5)}, got {result.shape}\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors found for `{moduleclass.__name__}`\")\n",
    "\n",
    "test_bert_classifier_module(BertClassifierModule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
