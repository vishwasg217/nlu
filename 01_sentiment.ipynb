{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import  Counter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 80488\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynasent_r1  = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r1.all', trust_remote_code=True)\n",
    "dynasent_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 14021\n",
      "\tneutral: 45076\n",
      "\tpositive: 21391\n",
      "validation\n",
      "\tnegative: 1200\n",
      "\tneutral: 1200\n",
      "\tpositive: 1200\n"
     ]
    }
   ],
   "source": [
    "def print_label_dist(dataset, label_name=\"gold_label\", split_names=(\"train\", \"validation\")):\n",
    "    for split_name in split_names:\n",
    "        print(split_name)\n",
    "        dist = sorted(Counter(dataset[split_name][label_name]).items())\n",
    "        for k, v in dist:\n",
    "            print(f\"\\t{k:>4s}: {v}\")\n",
    "\n",
    "print_label_dist(dynasent_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dynasent_r2 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r2.all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'r2-0000042',\n",
       " 'hit_ids': ['y22532', 'y22740'],\n",
       " 'sentence': 'He kindly walked us through the menu through the specialties and the wine list that we did not want.',\n",
       " 'sentence_author': 'w148',\n",
       " 'has_prompt': True,\n",
       " 'prompt_data': {'indices_into_review_text': [453, 525],\n",
       "  'review_rating': 5,\n",
       "  'prompt_sentence': 'He walked us through the menu through the specialties and the wine list.',\n",
       "  'review_id': '7joEfaGr4aC0OrR9I7CJ8Q'},\n",
       " 'model_1_label': 'positive',\n",
       " 'model_1_probs': {'negative': 0.15193994343280792,\n",
       "  'positive': 0.7109395265579224,\n",
       "  'neutral': 0.1371205449104309},\n",
       " 'text_id': 'r2-0000042',\n",
       " 'label_distribution': {'positive': [],\n",
       "  'negative': ['w516', 'w199', 'w294'],\n",
       "  'neutral': ['w544'],\n",
       "  'mixed': ['w527']},\n",
       " 'gold_label': 'negative',\n",
       " 'metadata': {'split': 'train',\n",
       "  'round': 2,\n",
       "  'subset': 'all',\n",
       "  'model_in_the_loop': 'RoBERTa'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynasent_r2['train'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 4579\n",
      "\tneutral: 2448\n",
      "\tpositive: 6038\n",
      "validation\n",
      "\tnegative: 240\n",
      "\tneutral: 240\n",
      "\tpositive: 240\n"
     ]
    }
   ],
   "source": [
    "print_label_dist(dynasent_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "sst = load_dataset(\"SetFit/sst5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 2218\n",
      "\tneutral: 1624\n",
      "\tpositive: 2322\n",
      "\tvery negative: 1092\n",
      "\tvery positive: 1288\n",
      "validation\n",
      "\tnegative: 289\n",
      "\tneutral: 229\n",
      "\tpositive: 279\n",
      "\tvery negative: 139\n",
      "\tvery positive: 165\n"
     ]
    }
   ],
   "source": [
    "print_label_dist(sst, label_name='label_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"a well-intentioned effort that 's still too burdened by the actor 's offbeat sensibilities for the earnest emotional core to emerge with any degree of accessibility .\",\n",
       " 'label': 1,\n",
       " 'label_text': 'negative'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst[\"train\"][35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reformart the SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\tnegative: 3310\n",
      "\tneutral: 1624\n",
      "\tpositive: 3610\n",
      "validation\n",
      "\tnegative: 428\n",
      "\tneutral: 229\n",
      "\tpositive: 444\n"
     ]
    }
   ],
   "source": [
    "def conver_sst_label(label):\n",
    "    return label.split(\" \")[-1]\n",
    "\n",
    "for split_name in (\"train\", \"test\", \"validation\"):\n",
    "    dist = [conver_sst_label(s) for s in sst[split_name]['label_text']]\n",
    "    sst[split_name] = sst[split_name].add_column(\"gold_label\", dist)\n",
    "    sst[split_name] = sst[split_name].add_column(\"sentence\", sst[split_name][\"text\"])\n",
    "\n",
    "print_label_dist(sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hello!': 1, 'i': 1, 'love': 1, 'machine': 1, 'learning!': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigrams_phi(sentence: str):\n",
    "    return Counter(sentence.lower().split(\" \"))\n",
    "\n",
    "uni = unigrams_phi(\"Hello! I love Machine Learning!\")\n",
    "uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c\n",
       "0  1.0  1.0  0.0\n",
       "1  0.0  1.0  2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = [\n",
    "    {'a': 1, 'b': 1},\n",
    "    {'b': 1, 'c': 2}\n",
    "]\n",
    "\n",
    "vec = DictVectorizer(sparse=False)\n",
    "X_train = vec.fit_transform(train_feats)\n",
    "\n",
    "df = pd.DataFrame(X_train, columns=vec.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c\n",
       "0  2.0  0.0  1.0\n",
       "1  4.0  2.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats = [\n",
    "    {'a': 2, 'c': 1},\n",
    "    {'a': 4, 'b': 2, 'd': 1}\n",
    "]\n",
    "\n",
    "X_test = vec.transform(test_feats)\n",
    "df = pd.DataFrame(X_test, columns=vec.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common mistake with DictVectorizer is calling fit_transform on test examples. This will wipe out the existing representation scheme, replacing it with one that matches the test examples. That will happen silently, but then you'll find that the new representations are incompatible with the model you fit. This is likely to manifest itself as a ValueError relating to feature counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Tweetgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Let': 1, 'that': 1, 'SINK': 1, 'in': 1, ':)': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tweetgrams_phi(sentence: str, **kwargs):\n",
    "    tk = TweetTokenizer(kwargs)\n",
    "    tokens = tk.tokenize(sentence)\n",
    "    return Counter(tokens)\n",
    "\n",
    "tweet = \"Let that SINK in :)\"\n",
    "\n",
    "grams = tweetgrams_phi(tweet, preserve_case=False)\n",
    "grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tweetgrams_phi(func):\n",
    "    examples = [\n",
    "        (\n",
    "            \"Here's an example with an emoticon :)\", \n",
    "            Counter({'an': 2, \"Here's\": 1, 'example': 1, 'with': 1, 'emoticon': 1, ':)': 1})\n",
    "        ),\n",
    "        (\n",
    "            \"The URL is https://pytorch.org!\", \n",
    "            Counter({'The': 1, 'URL': 1, 'is': 1, 'https://pytorch.org': 1, '!': 1})\n",
    "        )\n",
    "    ]\n",
    "    errcount = 0\n",
    "    for ex, expected in examples:\n",
    "        result = func(ex, preserve_case=True)\n",
    "        if result != expected:\n",
    "            errcount += 1\n",
    "            print(f\"Error for `{func.__name__}`: For input {ex}, \"\n",
    "                  f\"expected {expected} but got {result}\")\n",
    "    caps_ex = \"CAPS\"\n",
    "    caps_result = func(caps_ex, preserve_case=False)\n",
    "    caps_expected = Counter({\"CAPS\": 1})\n",
    "    if caps_result != caps_expected:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{func.__name__}`: For input {caps_ex}, \"\n",
    "              f\"expected {caps_expected} but got {caps_result}\")\n",
    "    if errcount == 0:\n",
    "        print(f\"All tests passed for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed for `tweetgrams_phi`\n"
     ]
    }
   ],
   "source": [
    "test_tweetgrams_phi(tweetgrams_phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), DictVectorizer())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_linear_model(model, featfunc, train_dataset):\n",
    "    \"\"\"Train an sklearn classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn classifier model\n",
    "    featfunc : func\n",
    "        Maps strings to Counter instances\n",
    "    train_dataset: dict\n",
    "        Must have a key \"sentence\" containing strings that `featfunc`\n",
    "        will process, and a key \"gold_label\" giving labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        * A trained version of `model`\n",
    "        * A fitted `vectorizer` for the train set\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    feats = [dict(featfunc(s)) for s in train_dataset['sentence']]\n",
    "    vec = DictVectorizer()\n",
    "    X_train = vec.fit_transform(feats)\n",
    "    model.fit(X_train, train_dataset['gold_label'])\n",
    "    return model, vec\n",
    "\n",
    "\n",
    "train_dataset = {\n",
    "    'sentence': ['A A', 'A B', 'B B', 'B A', 'B'],\n",
    "    'gold_label': [0, 1, 0, 1, 1]\n",
    "}\n",
    "def featfunc(s):\n",
    "    return Counter(s.split())\n",
    "\n",
    "model = LogisticRegression()\n",
    "train_linear_model(model, featfunc, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `train_linear_model`\n"
     ]
    }
   ],
   "source": [
    "def test_train_linear_model(func):\n",
    "    train_dataset = {\n",
    "        'sentence': ['A A', 'A B', 'B B', 'B A', 'B'],\n",
    "        'gold_label': [0, 1, 0, 1, 1]}\n",
    "    def featfunc(s):\n",
    "        return Counter(s.split())\n",
    "    model = LogisticRegression()\n",
    "    result = func(model, featfunc, train_dataset)\n",
    "    if not isinstance(result, tuple) or len(result) != 2:\n",
    "        print(f\"Error for `{func.__name__}`: Incorrect return type\")\n",
    "        return\n",
    "    model, vectorizer = result\n",
    "    if not hasattr(vectorizer, 'vocabulary_'):\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"Second return value is not a trained vectorizer\")\n",
    "        return\n",
    "    if not hasattr(model, 'classes_'):\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "              f\"First return value is not a trained classifier\")\n",
    "        return\n",
    "    print(f\"No errors found for `{func.__name__}`\")\n",
    "\n",
    "_ =  test_train_linear_model(train_linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model on dynasent R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_unigrams, vec_unigrams = train_linear_model(\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    unigrams_phi, dynasent_r1['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_linear_model(model, featfunc, vectorizer, assess_dataset):\n",
    "    \"\"\"Assess a trained sklearn model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: trained sklearn model\n",
    "    featfunc : func\n",
    "        Maps strings to count dicts\n",
    "    vectorizer : fitted DictVectorizer\n",
    "    assess_dataset: dict\n",
    "        Must have a key \"sentence\" containing strings that `featfunc`\n",
    "        will process, and a key \"gold_label\" giving labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A classification report (multiline string)\n",
    "\n",
    "    \"\"\"\n",
    "    feats = [featfunc(s) for s in assess_dataset['sentence']]\n",
    "    X_test = vectorizer.transform(feats)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    return classification_report(assess_dataset['gold_label'], preds, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found for `assess_linear_model`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishwasgowda/code/nlu/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/vishwasgowda/code/nlu/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/vishwasgowda/code/nlu/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def test_assess_linear_model(assessfunc, trainfunc):\n",
    "    train_dataset = {\n",
    "        'sentence': ['A A', 'A B', 'B B', 'B A', 'A', 'B'],\n",
    "        'gold_label': [0, 1, 0, 1, 0, 1]}\n",
    "    assess_dataset = {\n",
    "        'sentence': ['A C', 'B A'],\n",
    "        'gold_label': [0, 1]}\n",
    "    def featfunc(s):\n",
    "        return Counter(s.split())\n",
    "    model = LogisticRegression()\n",
    "    model, vectorizer = trainfunc(model, featfunc, train_dataset)\n",
    "    result = assessfunc(model, featfunc, vectorizer, assess_dataset)\n",
    "    errcount = 0\n",
    "    if len(vectorizer.vocabulary_) != 2:\n",
    "        print(f\"Error for `{assessfunc.__name__}`: Unexpected feature count\")\n",
    "        errcount += 1\n",
    "    if 'weighted avg' not in result:\n",
    "        print(f\"Error for `{assessfunc.__name__}`: Unexpected return value\")\n",
    "        errcount += 1\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors found for `{assessfunc.__name__}`\")\n",
    "\n",
    "test_assess_linear_model(assess_linear_model, train_linear_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.759     0.364     0.492      1200\n",
      "     neutral      0.523     0.890     0.659      1200\n",
      "    positive      0.699     0.572     0.629      1200\n",
      "\n",
      "    accuracy                          0.609      3600\n",
      "   macro avg      0.660     0.609     0.593      3600\n",
      "weighted avg      0.660     0.609     0.593      3600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = assess_linear_model(\n",
    "    lr_unigrams,\n",
    "    unigrams_phi,\n",
    "    vec_unigrams,\n",
    "    dynasent_r1['validation'])\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
